{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? `\n",
    "\n",
    "   Your answer: **Softmax Classifier tells us the probability of a particular sample belonging to a class. The values associated with each class for a sample add up to 1. \n",
    "   Now, if we want the classifier to work properly, the loss associated to a sample should be extremely high when the predicited value diverges from the actual value. (i.e. the probability of a sample belonging to the correct class is low)\n",
    "   The cross entropy does exactly this. For values between 0 and 1, the log value increases drastically as the distance between the correct class which has probablity 1 and the predicted probability (between 0 and 1) differ. \n",
    "   Thus the crossentropy helps to calculate the distance between the probability distribution and increases the loss significantly as probability diverges. It is thus used **\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model.\n",
    "\n",
    "   Your answer: **In binary logistic regression, we have to classify whether a sample lies in one class or inanother. In multiclass logistic regression, we have multiple classes. We have to classify the sample into one of the many classes that are available.\n",
    "   To derive a multiclass logistic regression from a binary regression, we can either use a one vs all method or a one vs one method.\n",
    "   In the one vs all method, we calculate the probability of each class and then select the class with the highest probability \n",
    "   In the one vs one method, we basically create a binary classifier, which compares one of the classes with all the other classes. Thus, we create n-1 classifiers for 1st class which are 1vs 2, 1vs3, 1vs4... ,1vs(n).   \n",
    "   Thus we create binary classisfiers for all such combinations of classes and we select that class which has the maximum wins.Thus we have to calculate nC2 binary classifiers for each sample. \n",
    "   In a deep learning model, we will have a Weights matrix of size(D, nC2), where D is dimension of sample, and nC2 which is the number of binary classifiers. After that we calculate the number of wins for each class. And choose the class which wins. We then calculate loss function using the number of wins and then gradients etc.\n",
    "   \n",
    "   **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: **Images mostly consist of nonlinear features. This includes edges or change in shapes etc. Sudden changes in the color should also be considered at this time. Our eyes and neurons do a very good job at separating and identifying these featuers. \n",
    "   The Relu function is very good at bringing out the non-linearity in images. The relu function also ends the problem of vanishing gradient. This is because the Relu function makes any value under 0 equal to 0. \n",
    "   Thus its used extensively in computer vision.**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "\n",
    "   Your answer: *The cross validation technique is used to evaluate a model quickly with very less data. It is a resampling type of method which constantly resamples the data\n",
    "   A small data set is chosen and dividen into k sub groups randomly. One of the k sub groups is used as the test group and the rest are added to the training_set.\n",
    "   Now we train the model and test it on the 1st test set. \n",
    "   We then choose the second of the K subgroups as the test set and measure the accuracy.\n",
    "   We then take the next subset of as the test set\n",
    "   We continue to select all the sub groups as test sets and train the model. This allows us to evealuate the model well\n",
    "   We can then change the parameter and try a different model. \n",
    "   *\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: **The two-layer model neural network was initially giving high losses. Thus the first step i did was to increasing the learning rate. However, That did not change the function much. Thus I felt that I should change the regularization parameter. It was pretty high. Thus the loss had less imporrance as compared to the weights of the parameters. As soon as I reduced the regularization I could see a suddent growth in the values. Next I started to view the training accuracy and the validation accuracy. As the difference between the training and validation accuracy was high at the end of epochs, I chose to increase the epochs as the model was undertrained. the loss function kept going down but stabilized. I also played around with the size of the hidden layers. This caused the loss function to dip more slowly as more parameters had to be tuned.**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: **[fill in here]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
