{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"task2-nmt.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"14pUU_2dyobW","colab_type":"text"},"source":["## Columbia University\n","### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."]},{"cell_type":"code","metadata":{"id":"JAGEJTvbypHv","colab_type":"code","outputId":"c46aa2ce-7435-419c-d029-016960f19bca","executionInfo":{"status":"ok","timestamp":1574915605319,"user_tz":300,"elapsed":20807,"user":{"displayName":"Abhijeet Arunkumar Mishra","photoUrl":"","userId":"02580029324233760324"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/DL-Assignment 3"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive/My Drive/DL-Assignment 3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hHUuy9D4yobY","colab_type":"text"},"source":["# Task 2: Neural Machine Translation:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-1aqdFlcyobZ","colab_type":"text"},"source":["The task of neural machine translation(NMT) implements multi-layer Recurrent Neural Network (RNN, LSTM, and GRU) for translating from one language to another. \n","\n","Here the task is to train a neural machine translation RNN in tensorflow to translate from french to english"]},{"cell_type":"code","metadata":{"id":"M8cbR6hByobZ","colab_type":"code","outputId":"a6b6680c-af68-4f54-93f4-be6b3aa37ecd","executionInfo":{"status":"ok","timestamp":1574920350571,"user_tz":300,"elapsed":491,"user":{"displayName":"Abhijeet Arunkumar Mishra","photoUrl":"","userId":"02580029324233760324"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["import os\n","import pickle\n","import copy\n","import numpy as np\n","import time\n","import tensorflow as tf\n","from utils.nmt import *\n","from matplotlib import pyplot as plt\n","\n","# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n","%load_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"},{"output_type":"stream","text":["[autoreload of utils.nmt failed: Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n","    superreload(m, reload, self.old_objects)\n","  File \"/gdrive/My Drive/DL-Assignment 3/utils/nmt.py\", line 420\n","    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, temp), grad_clip)\n","                                                                         ^\n","TabError: inconsistent use of tabs and spaces in indentation\n","]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"qFo0dX_Fyobb","colab_type":"text"},"source":["## Part 1: Setup \n","\n","Here will will preprocess the data necessary for the task"]},{"cell_type":"code","metadata":{"id":"9FPy_IzHyobc","colab_type":"code","colab":{}},"source":["source_path = 'nmt_data/fr.txt'\n","target_path = 'nmt_data/en.txt'\n","source_text = load_data(source_path)\n","target_text = load_data(target_path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WnJ9Imp2yobd","colab_type":"text"},"source":["Let us have a look at some sample translation to get an inderstanding of the task."]},{"cell_type":"code","metadata":{"id":"pC3u48UCyobe","colab_type":"code","outputId":"25326905-7e27-4869-82e5-6fb0ba727fbd","executionInfo":{"status":"ok","timestamp":1574920353297,"user_tz":300,"elapsed":1096,"user":{"displayName":"Abhijeet Arunkumar Mishra","photoUrl":"","userId":"02580029324233760324"}},"colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["english_sentences = target_text.split('\\n')\n","french_sentences = source_text.split('\\n')\n","\n","#Fetch the first five translations\n","sents = list(zip(french_sentences,english_sentences))[0:5]\n","print(sents)\n","side_by_side_sentences = list(zip(english_sentences, french_sentences))[0:5]\n","print(\"Sample translations: \\n\")\n","for index, sentence in enumerate(side_by_side_sentences):\n","    en_sent,fr_sent  = sentence\n","    print('sentence number {}'.format(index+1))\n","    print('\\tfr: {}'.format(fr_sent))\n","    print('\\ten: {}'.format(en_sent))\n","    print()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[(\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\", 'new jersey is sometimes quiet during autumn , and it is snowy in april .'), ('les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .', 'the united states is usually chilly during july , and it is usually freezing in november .'), ('california est généralement calme en mars , et il est généralement chaud en juin .', 'california is usually quiet during march , and it is usually hot in june .'), ('les états-unis est parfois légère en juin , et il fait froid en septembre .', 'the united states is sometimes mild during june , and it is cold in september .'), ('votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .', 'your least liked fruit is the grape , but my least liked is the apple .')]\n","Sample translations: \n","\n","sentence number 1\n","\tfr: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n","\ten: new jersey is sometimes quiet during autumn , and it is snowy in april .\n","\n","sentence number 2\n","\tfr: les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n","\ten: the united states is usually chilly during july , and it is usually freezing in november .\n","\n","sentence number 3\n","\tfr: california est généralement calme en mars , et il est généralement chaud en juin .\n","\ten: california is usually quiet during march , and it is usually hot in june .\n","\n","sentence number 4\n","\tfr: les états-unis est parfois légère en juin , et il fait froid en septembre .\n","\ten: the united states is sometimes mild during june , and it is cold in september .\n","\n","sentence number 5\n","\tfr: votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n","\ten: your least liked fruit is the grape , but my least liked is the apple .\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FAPjwkDXyobf","colab_type":"text"},"source":["Now that we have the translation data. We will do the following preprocessing steps"]},{"cell_type":"markdown","metadata":{"id":"dg9uDjdGyobg","colab_type":"text"},"source":["1. create lookup tables\n","    - here we create a unique mapping between each distinct word and it's word id\n","2. text to word ids\n","    - we convert all the text sentences to word id replaced sentences"]},{"cell_type":"code","metadata":{"id":"UUrNrLwEyobg","colab_type":"code","colab":{}},"source":["#please look at utils/nmt.py for the full function\n","preprocess_and_save_data(source_path, target_path, text_to_ids)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xUbqftCuyobi","colab_type":"code","outputId":"9bbc8eee-64e3-435e-b4ba-ef5ec260aebf","executionInfo":{"status":"ok","timestamp":1574920370594,"user_tz":300,"elapsed":3386,"user":{"displayName":"Abhijeet Arunkumar Mishra","photoUrl":"","userId":"02580029324233760324"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#load the data after it has been preprocessed\n","(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n","print(source_int_text[0], source_vocab_to_int)#, target_int_text, source_vocab_to_int, target_vocab_to_int)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[297, 224, 128, 106, 229, 239, 245, 100, 223, 176, 301, 128, 347, 298, 49, 145] {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3, 'as-tu': 4, 'pleut': 5, 'bleu': 6, 'frais': 7, 'va': 8, 'était': 9, 'du': 10, 'novembre': 11, 'votre': 12, 'aimé': 13, \"n'êtes\": 14, \"d'\": 15, 'visiter': 16, 'à': 17, 'lui': 18, 'vos': 19, 'états-unis': 20, 'citron': 21, 'juin': 22, 'chinois': 23, 'requins': 24, 'pomme': 25, 'dernier': 26, 'intention': 27, '-ce': 28, 'comment': 29, 'notre': 30, 'êtes-vous': 31, 'mois': 32, 'poire': 33, 'aller': 34, 'envisagent': 35, 'allée': 36, 'cette': 37, 'clémentes': 38, \"n'a\": 39, 'lac': 40, '?': 41, 'enneigée': 42, 'camion': 43, \"l'orange\": 44, 'pas': 45, 'peu': 46, 'raisins': 47, 'traduis': 48, 'avril': 49, 'france': 50, 'où': 51, 'aiment': 52, 'pousse': 53, 'beau': 54, 'allez': 55, 'faire': 56, 'pamplemousses': 57, 'oranges': 58, 'chaud': 59, 'mai': 60, 'déteste': 61, 'glaciales': 62, 'sont': 63, \"l'épicerie\": 64, 'aimons': 65, 'son': 66, 'animal': 67, 'trouvé': 68, 'cheval': 69, 'lapin': 70, 'vont': 71, 'trop': 72, 'nos': 73, 'bénigne': 74, 'préférée': 75, \"l'animal\": 76, 'moindres': 77, 'avons': 78, \"qu'elle\": 79, 'la': 80, 'redoutés': 81, \"l'oiseau\": 82, 'grosse': 83, 'maillot': 84, 'oiseaux': 85, 'agréable': 86, 'noir': 87, 'california': 88, 'petite': 89, 'allons': 90, \"l'école\": 91, 'voiture': 92, \"qu'il\": 93, 'aimé.': 94, 'blanche': 95, 'voulait': 96, 'chaude': 97, 'bananes': 98, 'blanc': 99, 'automne': 100, 'fruits': 101, 'pourraient': 102, 'pourrait': 103, \"j'aime\": 104, 'vit': 105, 'parfois': 106, 'printemps': 107, 'aimait': 108, 'sèche': 109, 'vers': 110, 'gèle': 111, 'neige': 112, 'chevaux': 113, 'souris': 114, 'jaune': 115, 'mangue': 116, 'au': 117, 'aiment-ils': 118, 'porcelaine': 119, 'pluies': 120, 'paris': 121, 'est-ce': 122, 'fraises': 123, 'grandes': 124, 'pluvieux': 125, 'rouillé': 126, 'brillant': 127, 'est': 128, 'leur': 129, \"l'automobile\": 130, 'ne': 131, 'petit': 132, \"n'aiment\": 133, 'plus': 134, \"n'aime\": 135, 'mangues': 136, 'automobile': 137, 'chaux': 138, 'que': 139, 'eiffel': 140, 'pêche': 141, 'verte': 142, 'singes': 143, 'traduction': 144, '.': 145, 'es-tu': 146, 'de': 147, 'janvier': 148, 'a': 149, 'gros': 150, 'grande': 151, 'voudrait': 152, 'visite': 153, 'aimée': 154, 'conduit': 155, 'singe': 156, 'fraise': 157, 'été': 158, 'des': 159, 'comme': 160, 'occupée': 161, 'habituellement': 162, 'apprécié': 163, 'ils': 164, 'grands': 165, 'cours': 166, 'verts': 167, 'conduite': 168, 'froid': 169, 'poires': 170, 'bleue': 171, 'prévoit': 172, 'aux': 173, 'redoutée': 174, 'voulaient': 175, 'et': 176, 'limes': 177, 'citrons': 178, 'doux': 179, 'merveilleux': 180, 'nouvelle': 181, 'généralement': 182, 'allions': 183, 'hiver': 184, 'redouté': 185, 'grosses': 186, 'serpent': 187, 'ont': 188, 'se': 189, 'inde': 190, 'nouveau': 191, \"n'aimait\": 192, 'grand': 193, 'mouillé': 194, 'prévoyons': 195, 'prochain': 196, 'détestez': 197, 'lions': 198, 'anglais': 199, 'manguiers': 200, 'pommes': 201, 'juillet': 202, \"l'éléphant\": 203, 'éléphant': 204, 'noire': 205, 'aime': 206, 'moins': 207, 'le': 208, 'veut': 209, 'cet': 210, 'douce': 211, 'français': 212, 'entre': 213, 'éléphants': 214, 'préférés': 215, 'gelés': 216, 'octobre': 217, 'banane': 218, '-elle': 219, 'humide': 220, 'gel': 221, 'espagnol': 222, ',': 223, 'jersey': 224, 'jamais': 225, '-ils': 226, 'sec': 227, 'cépage': 228, 'calme': 229, 'aimeraient': 230, 'chats': 231, 'voulez': 232, 'rouille': 233, 'chiens': 234, 'quand': 235, 'février': 236, 'californie': 237, 'fruit': 238, 'pendant': 239, 'détendre': 240, '-': 241, 'terrain': 242, 'serpents': 243, 'étaient': 244, \"l'\": 245, \"l'ours\": 246, 'ressort': 247, 'qui': 248, 'pêches': 249, 'gelé': 250, 'détend': 251, 'nous': 252, 'dernière': 253, 'vu': 254, 'allés': 255, 'une': 256, 'difficile': 257, 'brillante': 258, 'lion': 259, 'décembre': 260, 'tout': 261, 'occupé': 262, 'chine': 263, 'favori': 264, 'leurs': 265, 'durant': 266, 'rendre': 267, 'moteur': 268, \"c'est\": 269, 'aimés': 270, 'favoris': 271, 'rouillée': 272, 'prévois': 273, 'chien': 274, 'détestons': 275, 'prévoient': 276, 'mon': 277, 'vert': 278, 'facile': 279, \"n'aimez\": 280, 'vais': 281, 'amusant': 282, 'préféré.': 283, \"n'aimons\": 284, 'temps': 285, 'détestait': 286, 'congélation': 287, 'petites': 288, 'monde': 289, 'lapins': 290, '-il': 291, 'un': 292, 'septembre': 293, 'vieux': 294, 'tranquille': 295, 'bien': 296, 'new': 297, 'en': 298, 'mais': 299, 'traduire': 300, 'il': 301, 'plaît': 302, 'je': 303, 't': 304, 'raisin': 305, 'oiseau': 306, 'ours': 307, 'magnifique': 308, \"n'est\": 309, 'chat': 310, 'aimez': 311, 'pourquoi': 312, 'conduisait': 313, 'août': 314, 'at': 315, 'mars': 316, 'légère': 317, 'frisquet': 318, 'ce': 319, 'etats-unis': 320, 'rouge': 321, 'mouillée': 322, 'enneigé': 323, 'pamplemousse': 324, 'belle': 325, 'volant': 326, 'les': 327, 'ses': 328, 'cher': 329, 'souvent': 330, 'pensez': 331, \"l'automne\": 332, 'petits': 333, 'tour': 334, 'vieille': 335, 'animaux': 336, 'elle': 337, 'portugais': 338, 'sur': 339, 'préféré': 340, 'requin': 341, 'relaxant': 342, 'veulent': 343, 'proches': 344, 'allé': 345, 'mes': 346, 'neigeux': 347, 'vous': 348, 'pense': 349, 'pluie': 350, 'préférées': 351, 'football': 352, 'dans': 353, 'i': 354, 'avez': 355, 'envisage': 356, 'fait': 357}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iIW08Vu6yobj","colab_type":"text"},"source":["## Part 2: Create the RNN model"]},{"cell_type":"markdown","metadata":{"id":"wMbHe-SJyobk","colab_type":"text"},"source":["![img](img/seq2seq.jpg)"]},{"cell_type":"markdown","metadata":{"id":"Jxhzj1RVyobk","colab_type":"text"},"source":["The seq2seq learning model in this assignment is based on this paper: https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf"]},{"cell_type":"markdown","metadata":{"id":"Fy2gGa31yobl","colab_type":"text"},"source":["The translation model can be visualized in the simplest way as shown above"]},{"cell_type":"markdown","metadata":{"id":"Iq3trny4yobl","colab_type":"text"},"source":["Some useful functions to look up for seq to seq translation task are\n","\n","\n","\n","#### Encoder\n","- [`tf.contrib.layers.embed_sequence`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence)\n","\n","RNN layers\n","- [`tf.contrib.rnn.LSTMCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell)\n","  - creates an LSTM cell\n","- [`tf.contrib.rnn.GRUCell`](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/GRUCell)\n","  - creates an LSTM cell\n","- [`tf.contrib.rnn.DropoutWrapper`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper)\n","  - wraps a cell with keep probability value \n","- [`tf.contrib.rnn.MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell)\n","  - stacks multiple RNN (type) cells\n","  \n","Encoding model\n","- [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n","  - put Embedding layer and RNN layer(s) all together\n","\n","#### Decoder training\n","- [`tf.contrib.seq2seq.TrainingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/TrainingHelper)\n","  - TrainingHelper is where we pass the embeded input. As the name indicates, this is only a helper instance. This instance should be delivered to the BasicDecoder, which is the actual process of building the decoder model.\n","- [`tf.contrib.seq2seq.BasicDecoder`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder)\n","  - BasicDecoder builds the decoder model. It means it connects the RNN layer(s) on the decoder side and the input prepared by TrainingHelper.\n","- [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode)\n","  - dynamic_decode unrolls the decoder model so that actual prediction can be retrieved by BasicDecoder for each time steps.\n","  \n","#### Decoder inference\n","- [`tf.contrib.seq2seq.GreedyEmbeddingHelper`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/GreedyEmbeddingHelper)\n","  - GreedyEmbeddingHelper dynamically takes the output of the current step and give it to the next time step's input. In order to embed the each input result dynamically, embedding parameter(just bunch of weight values) should be provided. Along with it, GreedyEmbeddingHelper asks to give the `start_of_sequence_id` for the same amount as the batch size and `end_of_sequence_id`.\n","- [`tf.contrib.seq2seq.BasicDecoder`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BasicDecoder)\n","  - same as described in the training process section\n","- [`tf.contrib.seq2seq.dynamic_decode`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode)\n","  - same as described in the training process section"]},{"cell_type":"markdown","metadata":{"id":"onr9hjRqyobm","colab_type":"text"},"source":["We also use 4 special tokens for translation:\n","\n","    * <PAD>: to pad the sentence so all sentences are of the same length\n","    * <EOS>: to mark the end of sentence\n","    * <UNK>: to mark texts which are not in our dictionary\n","    * <GO>: the first token that is passed to the decoder output"]},{"cell_type":"markdown","metadata":{"id":"NvAvpDnuyobm","colab_type":"text"},"source":["<span style=\"color:red\">__TODO:__</span>: finish the following functions in utils/nmt.py. Refer to the functions above on which functions could be helpful\n","\n","    * encoding_layer : creates the enncoder part of the seq-seq learning architecture.\n","    * decoding_layer : creates the decoder part of the seq2seq learning architecture. This function outputs both the output during training and output during inference\n","    * my_optimizer : implements the optimizer with gradient clipping"]},{"cell_type":"markdown","metadata":{"id":"YvRxyR-pyobn","colab_type":"text"},"source":["Initially we will create a Seq2Seq model using LSTM as the building block. First we will will define all the hyperparameters required. Feel free to play around with them to improve the model performance"]},{"cell_type":"code","metadata":{"id":"89CBu1mVyobn","colab_type":"code","colab":{}},"source":["# these are preset parameters, you can change them to get better result\n","display_step = 300\n","\n","epochs = 13\n","batch_size = 128\n","\n","rnn_size = 128\n","num_layers = 3\n","\n","encoding_embedding_size = 200\n","decoding_embedding_size = 200\n","\n","learning_rate = 0.001\n","keep_probability = 0.5\n","grad_clip = 5\n","cell_type = 'LSTM'\n","rnn1_loss_history = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6v94gtL0q3L8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mOx4ZlHwyobp","colab_type":"text"},"source":["Next we will define the graph for the model."]},{"cell_type":"code","metadata":{"id":"GSbEgSmeyobp","colab_type":"code","colab":{}},"source":["save_path = 'checkpoints/dev_LSTM'\n","(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n","max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n","\n","train_graph = tf.Graph()\n","with train_graph.as_default():\n","    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n","    lr, keep_prob = hyperparam_inputs()\n","    \n","    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n","                                                   targets,\n","                                                   keep_prob,\n","                                                   batch_size,\n","                                                   target_sequence_length,\n","                                                   max_target_sequence_length,\n","                                                   len(source_vocab_to_int),\n","                                                   len(target_vocab_to_int),\n","                                                   encoding_embedding_size,\n","                                                   decoding_embedding_size,\n","                                                   rnn_size,\n","                                                   num_layers,\n","                                                   target_vocab_to_int,\n","                                                   cell_type)\n","    \n","    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n","    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n","    # - Returns a mask tensor representing the first N positions of each cell.\n","    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n","\n","    with tf.name_scope(\"optimization\"):\n","        # Loss function - weighted softmax cross entropy\n","        cost = tf.contrib.seq2seq.sequence_loss(\n","            training_logits,\n","            targets,\n","            masks)\n","\n","        # Optimizer\n","        train_op = my_optimizer(cost,grad_clip,lr)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isgnl4G8yobq","colab_type":"text"},"source":["Once the graph has been defined we will train the seq2seq model to perform the translations"]},{"cell_type":"code","metadata":{"id":"EeWgxLlZyobr","colab_type":"code","outputId":"3bb53c2d-355b-4d2a-9857-03bf4ceba18b","executionInfo":{"status":"ok","timestamp":1574922699500,"user_tz":300,"elapsed":1467257,"user":{"displayName":"Abhijeet Arunkumar Mishra","photoUrl":"","userId":"02580029324233760324"}},"colab":{"base_uri":"https://localhost:8080/","height":918}},"source":["\n","\n","# Split data to training and validation sets\n","train_source = source_int_text[batch_size:]\n","train_target = target_int_text[batch_size:]\n","valid_source = source_int_text[:batch_size]\n","valid_target = target_int_text[:batch_size]\n","(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n","                                                                                                             valid_target,\n","                                                                                                             batch_size,\n","                                                                                                             source_vocab_to_int['<PAD>'],\n","                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n","\n","with tf.Session(graph=train_graph) as sess:\n","    sess.run(tf.global_variables_initializer())\n","\n","    for epoch_i in range(epochs):\n","        print(epoch_i)\n","        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n","                get_batches(train_source, train_target, batch_size,\n","                            source_vocab_to_int['<PAD>'],\n","                            target_vocab_to_int['<PAD>'])):\n","            _, loss = sess.run(\n","                [train_op, cost],\n","                {input_data: source_batch,\n","                 targets: target_batch,\n","                 lr: learning_rate,\n","                 target_sequence_length: targets_lengths,\n","                 keep_prob: keep_probability})\n","            rnn1_loss_history.append(loss)\n","            \n","\n","            if batch_i % display_step == 0 and batch_i > 0:\n","                batch_train_logits = sess.run(\n","                    inference_logits,\n","                    {input_data: source_batch,\n","                     target_sequence_length: targets_lengths,\n","                     keep_prob: 1.0})\n","\n","                batch_valid_logits = sess.run(\n","                    inference_logits,\n","                    {input_data: valid_sources_batch,\n","                     target_sequence_length: valid_targets_lengths,\n","                     keep_prob: 1.0})\n","\n","                train_acc = get_accuracy(target_batch, batch_train_logits)\n","                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n","\n","                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n","                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n","\n","    # Save Model\n","    saver = tf.train.Saver()\n","    saver.save(sess, save_path)\n","    print('Model Trained and Saved')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","Epoch   0 Batch  300/1077 - Train Accuracy: 0.6285, Validation Accuracy: 0.6363, Loss: 1.5075\n","Epoch   0 Batch  600/1077 - Train Accuracy: 0.6602, Validation Accuracy: 0.6450, Loss: 1.0458\n","Epoch   0 Batch  900/1077 - Train Accuracy: 0.6858, Validation Accuracy: 0.6992, Loss: 0.8574\n","1\n","Epoch   1 Batch  300/1077 - Train Accuracy: 0.7405, Validation Accuracy: 0.7378, Loss: 0.7095\n","Epoch   1 Batch  600/1077 - Train Accuracy: 0.7483, Validation Accuracy: 0.7526, Loss: 0.6511\n","Epoch   1 Batch  900/1077 - Train Accuracy: 0.7574, Validation Accuracy: 0.7578, Loss: 0.6237\n","2\n","Epoch   2 Batch  300/1077 - Train Accuracy: 0.7856, Validation Accuracy: 0.7695, Loss: 0.5636\n","Epoch   2 Batch  600/1077 - Train Accuracy: 0.7869, Validation Accuracy: 0.7873, Loss: 0.5048\n","Epoch   2 Batch  900/1077 - Train Accuracy: 0.7908, Validation Accuracy: 0.7934, Loss: 0.5008\n","3\n","Epoch   3 Batch  300/1077 - Train Accuracy: 0.8099, Validation Accuracy: 0.8043, Loss: 0.4288\n","Epoch   3 Batch  600/1077 - Train Accuracy: 0.8242, Validation Accuracy: 0.8142, Loss: 0.4043\n","Epoch   3 Batch  900/1077 - Train Accuracy: 0.8125, Validation Accuracy: 0.8177, Loss: 0.4177\n","4\n","Epoch   4 Batch  300/1077 - Train Accuracy: 0.8416, Validation Accuracy: 0.8351, Loss: 0.3587\n","Epoch   4 Batch  600/1077 - Train Accuracy: 0.8594, Validation Accuracy: 0.8459, Loss: 0.3247\n","Epoch   4 Batch  900/1077 - Train Accuracy: 0.8537, Validation Accuracy: 0.8520, Loss: 0.3188\n","5\n","Epoch   5 Batch  300/1077 - Train Accuracy: 0.8798, Validation Accuracy: 0.8650, Loss: 0.2696\n","Epoch   5 Batch  600/1077 - Train Accuracy: 0.8763, Validation Accuracy: 0.8724, Loss: 0.2608\n","Epoch   5 Batch  900/1077 - Train Accuracy: 0.8780, Validation Accuracy: 0.8850, Loss: 0.2599\n","6\n","Epoch   6 Batch  300/1077 - Train Accuracy: 0.8963, Validation Accuracy: 0.8984, Loss: 0.2190\n","Epoch   6 Batch  600/1077 - Train Accuracy: 0.9036, Validation Accuracy: 0.9054, Loss: 0.2128\n","Epoch   6 Batch  900/1077 - Train Accuracy: 0.9019, Validation Accuracy: 0.9132, Loss: 0.2017\n","7\n","Epoch   7 Batch  300/1077 - Train Accuracy: 0.9210, Validation Accuracy: 0.9227, Loss: 0.1663\n","Epoch   7 Batch  600/1077 - Train Accuracy: 0.9310, Validation Accuracy: 0.9306, Loss: 0.1548\n","Epoch   7 Batch  900/1077 - Train Accuracy: 0.9327, Validation Accuracy: 0.9410, Loss: 0.1489\n","8\n","Epoch   8 Batch  300/1077 - Train Accuracy: 0.9449, Validation Accuracy: 0.9440, Loss: 0.1206\n","Epoch   8 Batch  600/1077 - Train Accuracy: 0.9410, Validation Accuracy: 0.9379, Loss: 0.1215\n","Epoch   8 Batch  900/1077 - Train Accuracy: 0.9379, Validation Accuracy: 0.9457, Loss: 0.1244\n","9\n","Epoch   9 Batch  300/1077 - Train Accuracy: 0.9531, Validation Accuracy: 0.9549, Loss: 0.1001\n","Epoch   9 Batch  600/1077 - Train Accuracy: 0.9531, Validation Accuracy: 0.9501, Loss: 0.1045\n","Epoch   9 Batch  900/1077 - Train Accuracy: 0.9527, Validation Accuracy: 0.9562, Loss: 0.1054\n","10\n","Epoch  10 Batch  300/1077 - Train Accuracy: 0.9592, Validation Accuracy: 0.9605, Loss: 0.0887\n","Epoch  10 Batch  600/1077 - Train Accuracy: 0.9596, Validation Accuracy: 0.9583, Loss: 0.0886\n","Epoch  10 Batch  900/1077 - Train Accuracy: 0.9596, Validation Accuracy: 0.9579, Loss: 0.0908\n","11\n","Epoch  11 Batch  300/1077 - Train Accuracy: 0.9605, Validation Accuracy: 0.9614, Loss: 0.0720\n","Epoch  11 Batch  600/1077 - Train Accuracy: 0.9627, Validation Accuracy: 0.9622, Loss: 0.0699\n","Epoch  11 Batch  900/1077 - Train Accuracy: 0.9648, Validation Accuracy: 0.9661, Loss: 0.0782\n","12\n","Epoch  12 Batch  300/1077 - Train Accuracy: 0.9674, Validation Accuracy: 0.9640, Loss: 0.0555\n","Epoch  12 Batch  600/1077 - Train Accuracy: 0.9670, Validation Accuracy: 0.9679, Loss: 0.0610\n","Epoch  12 Batch  900/1077 - Train Accuracy: 0.9644, Validation Accuracy: 0.9666, Loss: 0.0724\n","Model Trained and Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O-uoq1LPyobs","colab_type":"code","colab":{}},"source":["# Save parameters for checkpoint\n","save_params(save_path,cell_type)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxizRBokyobu","colab_type":"code","outputId":"b7b6b746-e8eb-4718-b336-205b46619f50","executionInfo":{"status":"ok","timestamp":1574923643329,"user_tz":300,"elapsed":940,"user":{"displayName":"Abhijeet Arunkumar Mishra","photoUrl":"","userId":"02580029324233760324"}},"colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["# plot loss history\n","loss_data = rnn1_loss_history\n","from matplotlib import pyplot as plt\n","plt.plot(np.arange(len(loss_data)),loss_data)\n","plt.ylabel('Training_loss')\n","plt.xlabel('EPOCHS')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAeLUlEQVR4nO3deXwddb3/8dfnZG3adA8l0JYChUpR\nCiXsLoCy88OHykXFiwvcR1Vwu3r1FtyXK4ob8vupWAUvKoiIgP6QHQQvgkAqBVq67y1L031Jm+Tk\nfO4fM0mznCQnJ2cyJ5P38/E4j8yZOWfmk0nyPpPvfOc75u6IiEhypeIuQEREoqWgFxFJOAW9iEjC\nKehFRBJOQS8iknClcRfQ0cSJE33atGlxlyEiMqTMnz9/s7vX9LS8qIJ+2rRp1NfXx12GiMiQYmZr\ne1uuphsRkYRT0IuIJJyCXkQk4RT0IiIJp6AXEUk4Bb2ISMIp6EVEEi4RQb+vpZUfPrSU+jVb4y5F\nRKToJCLo0xnnhsdW8Py67XGXIiJSdBIR9CkLvmZ0ExURkW4SEvRB0meU8yIi3SQi6E1H9CIiPUpE\n0Lcd0ev+tyIi3SUi6MMDejXdiIhkkYig339EH3MhIiJFKBFBrzZ6EZGeJSToDTO10YuIZJOIoIeg\n+UZt9CIi3SUo6NV0IyKSTWKC3tARvYhINskJegNHSS8i0lVigj5lpu6VIiJZlEa9ATNbA+wCWoG0\nu9dFsZ2UQUZtNyIi3UQe9KEz3H1zlBtQrxsRkewS03Rj6nUjIpLVYAS9Aw+Z2Xwzm9N1oZnNMbN6\nM6tvaGjIeyNmpgumRESyGIygf7O7zwbOA64ys7d2XOju89y9zt3rampq8t5IylCfGxGRLCIPenff\nGH7dBNwNnBjFdoI2ekW9iEhXkQa9mY00s+q2aeBsYGFE29LJWBGRLKLudTMJuNuC4SVLgdvc/YEo\nNpTSoGYiIllFGvTuvgqYFeU22qTMyGQGY0siIkOLuleKiCRcYoJeF0yJiGSXmKDXoGYiItklJug1\nqJmISHYJCnq10YuIZJOgoFcbvYhINokJevW6ERHJLkFBr0HNRESySUzQB1fGxl2FiEjxSVDQa1Az\nEZFsEhP0GtRMRCS7xAS9BjUTEckuQUGvI3oRkWwSE/TqXikikl2Cgl5DIIiIZJOYoNcQCCIi2SUo\n6HVELyKSTYKCXkf0IiLZJCboTRdMiYhklZygB3WvFBHJIjFBnwpuMSUiIl0kJ+hTaqMXEckmOUGv\nNnoRkawSE/Qa1ExEJLvEBL0GNRMRyS4xQa9eNyIi2SUm6FNmuLrdiIh0MyhBb2YlZva8md0b4TbI\nZKJau4jI0DVYR/SfBhZHuQENgSAikl3kQW9mk4ELgF9GuR0NaiYikt1gHNFfD3wByNqwYmZzzKze\nzOobGhry3ogumBIRyS7SoDezC4FN7j6/p9e4+zx3r3P3upqamvy3hS6YEhHJJuoj+tOAi8xsDXA7\ncKaZ/TaKDWmoGxGR7CINene/2t0nu/s04H3AY+7+r1FsS230IiLZJagfvdroRUSyKR2sDbn748Dj\nUa1fg5qJiGSXmCN6XTAlIpJdgoJeg5qJiGSTmKBPqdeNiEhWCQp6tdGLiGSTmKDXjUdERLJLTNDr\nxiMiItklKOh1RC8ikk1igt50wZSISFaJCXoNgSAikl1igl5H9CIi2eUU9GZ2rZmNNrNSM3vQzF43\ns0ujLq4/dEQvIpJdrkf057n7TuBC4BXgDcB/RlZVHjSomYhIdrkGfdvgZ+cDf3D3bRTZhai79qVp\nbG6NuwwRkaKT6+iV95vZQqAVuMrMJgJN0ZXVf7c/tx6ATMZJpSzmakREikdOR/Tu/nngTOB4d28B\n9gLvjrKw/jp9RnAbwrQ604uIdJLrydh3A3vdPW1mc4FfAfnf4DUCJx46HlA7vYhIV7m20X/N3XeZ\n2akE7fS3AjdGV1b/lVjQXNOqI3oRkU5yDfq2s5wXAj939z8BFdGUlJ+SsF2+VUf0IiKd5Hoy9lUz\n+wlwHnC8mZVTZBdbpcIj+oyO6EVEOsk1rC8BngDOD7tWTgTmRlZVHtqP6BX0IiKd5NrrZjewCDjd\nzD4GjHP3+yOtrJ9SaroREckq1143nwD+AEwNH3eY2ZVRFtZfizbuAKBhV1F17xcRiV2ubfRzgBPD\nI3vM7NvAU8BPoyqsv9oumHpqxRaOPmhMzNWIiBSPXNvoDWju8LwlnFd01HQjItJZrkf0vwGeMbM/\nhs/fBdwSTUn5mTiqnM27mxlRVhJ3KSIiRSXXk7HXAR8FGsPHx9z9+1EW1l/XnH8UALVjKmOuRESk\nuPQa9OEY9KPNbDSwBPhl+Fgazisajy3ZBMCc38yPuRIRkeLSV9PNIoLhiNva49sawC2cnhpRXf22\n/PXdcZcgIlKUeg16d5+Sy0rM7A3uviTL/ErgbwTDJZQCd7r7V/MptC97WzQWvYhINoUaxuC2HuY3\nAWe6+yzgWOBcMzu5QNvs5OLjJ0exWhGRIa9QQZ+1q6UH2tpUysJHJP0fLzv5kKCQouz0KSISn0IF\nfY/hbWYlZrYA2AQ87O7PdFk+x8zqzay+oaEh7wLaBjVTN3oRkc4iH4HS3Vvd/VhgMnCimb2xy/J5\n7l7n7nU1Nfnfy6QlkxlgpSIiyVSooO/zTKi7bwf+CpxboG12Ul0ZnFceM6IsitWLiAxZOV0Za2bH\nZJm9A1jv7hl3P6GH99UALe6+3cxGAGcB38272l5UlJaE24xi7SIiQ1euQyDcRNBrZhHBidejgJeB\najOb4+6P9vC+WuAWMysh+O/hDne/d4A192p7Y0uUqxcRGXJyDfo1wBXu/iKAmb0J+DJwDXAnwYdA\nN+Hrjxt4mSIikq9c2+iPagt5AHd/CZjp7iuiKUtERAol1yP6JWb2f4Hbw+fvDedVAOlIKhMRkYLI\n9Yj+g8AGgvvEzgVeAT5EEPJvj6a0/Lk604uItMvpiN7dGwl6y2TrMbOjoBUVwPbGFsaNLI+7DBGR\nopBr98qTga8Ch3R8j7sfGVFdA9LSqounRETa5Np08yuC+8O+A3hLh0dRueCYWgAeWbwp5kpERIpH\nrkG/093/v7u/4u6vtz0irSwPf3nxVQCuufulmCsRESkeufa6eczMrgXuIhh6GGjvJy8iIkUs16B/\nc5evEIxY+dbCliMiIoWWa6+bomuPz2bW5DG8sKHoOgGJiMSq16A3s/e7++/M7FPZlrv7DdGUlZ9j\np4xV0IuIdNHXEf248Gv+A8UPoqqKXFuiRESGj75uDv7T8OuXB6ecgTl47Ii4SxARKTq5XjA1Ebgc\nmEbnC6bmRFNWfs5744F86Z6FcZchIlJUcm3r+BPwD+BJcribVFwmjKqIuwQRkaKTa9CPdPfPRVqJ\niIhEItcrY+83s7MjraTAduzVnaZERCD3oP8Y8ICZ7TazrWa2zcy2RlnYQD248LW4SxARKQq5Nt1M\njLSKCDTsbur7RSIiw0CvR/RmdkQ4eXQPj6L1vQeXxl2CiEhR6OuIfi5wBfCTLMs01o2IyBDQ1wVT\nV4Rfh8RYNyIi0l3OYwaY2RuAmUBl2zx3vy2KogaiojRFU1p3mBIRaZNTrxsz+xIwD7gROA+4Hrg4\nwrry9o+ri+5e5SIiscq1e+V7gTOAV939MmAWMDKyqgZANwUXEeks16Df6+6tQNrMqoHXCG4UXtSa\n0kU7WoOIyKDJNeifN7OxwM1APfBs+Chqdzy3Pu4SRERi1+fJWDMz4Gvuvh34iZk9CIx293/m8N4p\nwK+BSQTdMee5+48HWHPO9rXopKyISJ9H9O7uwMMdnq/IJeRDaeBz7j4TOBm4ysxm5lVpHh5cpGEQ\nRERybbpZYGbH9Xfl7v5q24eCu+8CFgMH93c9+apfu22wNiUiUrT6umdsqbungeOA58xsJbAHMIKD\n/dm5bsjMpoXreSbvakVEpN/6aqN/FpgNXDSQjZjZKOCPwGfcfWeXZXOAOQBTp04dyGbaVVeWsmtf\nuiDrEhEZ6vpqujEAd1+Z7ZHLBsysjCDkb3X3u7oud/d57l7n7nU1NYW5B/mn335E3y8SERkm+jqi\nrzGzz/a00N1/2Nubwx47NwGL+3ptIb3z2IP51l8WD9bmRESKWl9BXwKMIjyyz8NpwGXAS2a2IJx3\njbvfl+f6clJdmfMQPiIiiddXIr7q7t/Id+Xu/iT5f0jkLWX7N9mczlBemmvnIhGR5MmpjX6oKU3t\nL/vFDdtjrEREJH59Bf2QHAoy1SHon165JcZKRETi12vQu3tR3wA8Fz94eFncJYiIxEqN1yIiCaeg\nFxFJOAW9iEjCJTboLzymNu4SRESKQmKD/iOnHRp3CSIiRSGxQT976tj26ZO+/UiMlYiIxCuxQW8d\nro59fWdTjJWIiMQrsUEvIiKBYRP06VbdP1ZEhqdhE/SX/PzpuEsQEYlFooN++gGj2qf/uU6Dm4nI\n8JTooP/jx0+NuwQRkdglOujHjCjr9Lx+zZAfo01EpN8SHfRdXXyj2ulFZPhJfNCPqyrr+0UiIgmW\n+KC/+cMnxF2CiEisEh/0x00d1+n5f975YkyViIjEI/FBD3BU7ej26d/Xr2dHY0uM1YiIDK5hEfT3\nf/otnZ7P+sZDMVUiIjL4hkXQi4gMZ8Mm6C+pm9zp+TfvfTmmSkREBtewCfrrLp7V6flNT64mk/GY\nqhERGTzDJuiz+eqfF8VdgohI5IZV0N/3qc4nZX/zj7U0NqdjqkZEZHAMq6CfedDo7vO+8mAMlYiI\nDJ5Ig97MbjazTWa2MMrt9MfCr5/TbV6LbkoiIgkW9RH9fwPnRryNfhlVUcrZMyd1mve5O16IqRoR\nkehFGvTu/jeg6MYGnvfBuk7P//zCK0yb+5eYqhERiVbsbfRmNsfM6s2svqGhYdC2O7O2e3u9iEgS\nxR707j7P3evcva6mpmbQtnvXlafytiM7b+8Ld6oJR0SSJ/agj0tlWQm3XH5ip3l31G/gxP96RBdS\niUiiDNugb7Pq2+d3er5pVxOHXXMfP3t8ZUwViYgUVtTdK38HPA3MMLMNZnZFlNvLRypl3PZvJ3Wb\n/90HlnDLU2sGvyARkQKLutfN+9291t3L3H2yu98U5fbyder0iVnnf/XPi2hOq4+9iAxtw77pps19\nn3oLd3z0lG7zj/zS/bznZ0+xvbGZV7bv5bhvPMTS13bFUKGISH7MvXhOPNbV1Xl9fX2sNTSlW5nx\npQd6fc2xU8Zyz1WnDVJFIiK9M7P57l7X03Id0XdRUVrC6mvPZ9qEqh5fs2D9dmZ/82HWb21kT5MG\nRROR4qYj+h40pzOc9aMnWLulMef3/H3umextbmX6AaM6zW/NOCkDMyt0mSIifR7RK+hz8OCi1/jo\nb+b36z0Hjq7kqblnsq2xmeO/9QifP2cGV50xPaIKRWQ4U9AXyI7GFp5csZlf/X019Wu35bWOIw4Y\nxY/eeyx/XbKJK8+YTklKR/giMnAK+gi4Ow27m3h+3XaeXb2Vm55cnfe63n/iFE6YNp6p46tYt7WR\nC46ppaK0pIDVikjSKegHyY69Lfzib6t4dvVWnl0zsAE7F3zlLMZWlReoMhFJOgV9DNZvbWTCqHKq\nykt5dcdeTrn2sbzXddnJh/DlC2eybmtjt5O8IiKgoC8KTelWSswoLQl6s67dsoe3fe/xAa3z4X9/\nK0dMqi5AdSIy1Cnoi1hLawYDSktSrGrYzZk/eCLvdb38jXOoKi8tXHEiMmQo6IeQTMZJpYxMxmls\naeV3z6zjv+5bnNe6Fn39HCpKU5SkTP33RRJOQZ8g9Wu2cuf8Ddz+3Pp+v/eiWQdx/psO5O1HTaKs\nRBdEiySJgj6h1mzewys79nLpL54p2Dq/cO4MDhozgiMnVTPzoNEs3LiDjDtHTqqmsqyE1Zv3UDum\nksoydf8UKSYK+mEk3Zrhhw8v46cR3zRl3mXHs25rI6Mry5h9yDieWNbARbMOoqa6ItLtikh2Cvph\nKpNxmlsz/M/yzYwfWcZ7fvb0oG37s2cdySOLX+cnl87mnuc38uHTpjGyvJTn12/j+EPGD1odIsOF\ngl66aWxOk844z63eyt9XbOHmv+d/ZW9/VZWXsOjr5+gEsUgBKeilX7Y3NlNdWUZJymhKt3Ln/A08\nvrSBC4+pZcLICv71psKdEwCorizlxa+ereAXGQAFvUTO3Xl65RbWbm3k7uc38uzqgQ0BMWZEGadN\nn8BFsw5mbFUZs6eOo6ykezfR3U1pfvjQMj5++uE6PyDDmoJeYtP2u/Xazn3c/ux6fvzo8si3+Ykz\npnPEpFE8sayBow4czSUnTKEp3cqqhj0cUF3BYTWjcHcyTr9HD21sTlNZWkJKo45KkVHQS1HatqeZ\nxa/u5NJfFrYpKF8fe9vhvHn6RB5bsomlr+/kt1ec1L7MzNrHLCovTbHsW+fFWKlIdwp6GVKa0xlW\nbNrNo4tf5wcPL4u7nB5dfPxkzphxAIdOHEljc5qjDxrDiHJdXyDxUNBLorS0Zti5t4UJoypY1bCb\nA8dUAvDK9n38+uk1rGrYQ0nKeGJZQ7yFhiaMLOfad7+JE6aNZ2RFKSsbdlNVXsLYEeWMqSqLuzxJ\nCAW9SBZPrdhMeWmKN9SO5tRrH2XnvuK5yftPPzCb06ZPZERZCWUlxrbGFspKjOrK3D8Ynl+3jaNq\nR3e7itndaWl1yks1DEZPnl65hX3pVs6YcUDcpeRMQS+SB3dv7+XTlG5lw7a9jCgroXZMJQ+9/Hq/\n7yEcpZTBeW+s5b6Fr+IOl9RN5o76DQB8/pwZzJ46jpm1o3luzVbuffEV7lnwCk9ffSa1Y0YAsK+l\nldJUMIy2u7O7Kd2vD5WkmTb3LwCs+c4FMVeSOwW9yCDZ3thMxoMPiXsWvMKph0/gyeWb8x6BdCg4\nZEIVu/almTK+ihfWb+f7/zKLMSPKuO6BJRw3dSytGfjexcfwxPIGylIpDq0ZycFjR7S/f9OufazZ\n3MgB1RXs2pfm8ANGdhpue9ueZo775sN89z1v4r0nTO21ll37Wvr8gNrdlGZURe/DeSvoI6agl+Hu\npQ07uOq2f7Jua2PcpSRCyiDTJeJGV5a2N9UdMqGKaRNGsn5rI598+3TGjCjj8v8OMugdR03iyjMO\nx4CJoyrIuJMy4y3X/bXTB8+zq7cyYVQ5U8ZVUZKyrN1297W0kjKLrMks9qA3s3OBHwMlwC/d/Ts9\nvVZBL9K3PU1pqspLMDNe27GPltYMCzfuoLw0xQmHjmfjtr2s3bKHCaMqWLlpNzXVFfz2H2uZMr6K\nXz+9Nu7ypRf5/hcRa9CbWQmwDDgL2AA8B7zf3V/O9noFvUhxy2ScdMbJuNPSmmHbnhYOGF1BScpo\nbGqlqqKEf67dxpTxVYwfWc66rY2s2byHp1Zu4ayZk7ijfj17mtI8sngTI8tL2NPcGve3VFSiCvqo\n7z13IrDC3VeFxdwOvBPIGvQiUtxSKaM8bJqoLCvp1CY+pipoljjpsAnt846cVM2Rk6o5++gDATht\n+sRBrDZ37k5rxtnT3MqYEcH39PrOfVRXBhH5wvodjKwoYfPuJv5Qv4GykhQnHzaBGx5dzsyDRtPS\nmiHd6jy9akt7c9EbDx7Nwo07ASgvTdGczgBwWM1IVjXs6VbDLZefGNn3F3XQHwx0vB3SBuCkHl4r\nIhILM6O0xBgzYn8b+qTRle3Tpxy+/8PrzDdMap++9KTeTxAXi9g705rZHDOrN7P6hobiuMhFRCRJ\nog76jcCUDs8nh/Paufs8d69z97qampqIyxERGX6iDvrngCPM7FAzKwfeB/w54m2KiEgHkbbRu3va\nzD4BPEjQvfJmd18U5TZFRKSzqE/G4u73AfdFvR0REcku9pOxIiISLQW9iEjCKehFRBKuqAY1M7MG\nYCCDcUwENheonKgNpVphaNU7lGqFoVXvUKoVhla9A6n1EHfvsX96UQX9QJlZfW/jPRSToVQrDK16\nh1KtMLTqHUq1wtCqN8pa1XQjIpJwCnoRkYRLWtDPi7uAfhhKtcLQqnco1QpDq96hVCsMrXojqzVR\nbfQiItJd0o7oRUSkCwW9iEjCJSLozexcM1tqZivMbG5MNUwxs7+a2ctmtsjMPh3OH29mD5vZ8vDr\nuHC+mdkNYc0vmtnsDuv6UPj65Wb2oYjrLjGz583s3vD5oWb2TFjX78NRRzGzivD5inD5tA7ruDqc\nv9TMzomozrFmdqeZLTGzxWZ2SjHvWzP79/D3YKGZ/c7MKotp35rZzWa2ycwWdphXsP1pZseb2Uvh\ne24ws+53zB5Yrd8LfxdeNLO7zWxsh2VZ91lPOdHTz6WQ9XZY9jkzczObGD4fnH3r7kP6QTAq5krg\nMKAceAGYGUMdtcDscLqa4F65M4HrgLnh/LnAd8Pp84H7AQNOBp4J548HVoVfx4XT4yKs+7PAbcC9\n4fM7gPeF0zcCHw+nrwRuDKffB/w+nJ4Z7vMK4NDwZ1ESQZ23AP8WTpcDY4t13xLcWW01MKLDPv1w\nMe1b4K3AbGBhh3kF25/As+FrLXzveQWu9WygNJz+bodas+4zesmJnn4uhaw3nD+FYCTftcDEwdy3\nkYTHYD6AU4AHOzy/Gri6COr6E8FN0ZcCteG8WmBpOP1zghult71+abj8/cDPO8zv9LoC1zgZeBQ4\nE7g3/MXZ3OEPqH3fhr+gp4TTpeHrrOv+7vi6AtY5hiA4rcv8oty37L+F5vhwX90LnFNs+xaYRufw\nLMj+DJct6TC/0+sKUWuXZe8Cbg2ns+4zesiJ3n7nC10vcCcwC1jD/qAflH2bhKabbPelPTimWgAI\n//U+DngGmOTur4aLXgPabjjZU92D+f1cD3wByITPJwDb3T2dZdvtdYXLd4SvH4x6DwUagF9Z0Mz0\nSzMbSZHuW3ffCHwfWAe8SrCv5lOc+7ajQu3Pg8PprvOjcjnBkS191JRtfm+/8wVjZu8ENrr7C10W\nDcq+TULQFxUzGwX8EfiMu+/suMyDj+Ci6M9qZhcCm9x9fty15KCU4F/hn7n7ccAegqaFdkW2b8cB\n7yT4gDoIGAmcG2tR/VRM+7M3ZvZFIA3cGnctPTGzKuAa4Ctx1ZCEoO/zvrSDxczKCEL+Vne/K5z9\nupnVhstrgU3h/J7qHqzv5zTgIjNbA9xO0HzzY2CsmbXdkKbjttvrCpePAbYMUr0bgA3u/kz4/E6C\n4C/WffsOYLW7N7h7C3AXwf4uxn3bUaH258Zwuuv8gjKzDwMXAh8IP5jyqXULPf9cCuVwgg/9F8K/\nt8nAP83swDzqzW/fFqq9L64HwdHeqnBHtp1kOTqGOgz4NXB9l/nfo/MJruvC6QvofBLm2XD+eIL2\n6HHhYzUwPuLaT2f/ydg/0PnE1JXh9FV0PmF4Rzh9NJ1Pfq0impOx/wPMCKe/Fu7Xoty3wEnAIqAq\nrOEW4JPFtm/p3kZfsP1J9xOG5xe41nOBl4GaLq/Lus/oJSd6+rkUst4uy9awv41+UPZtZOExmA+C\nM9fLCM6qfzGmGt5M8K/ui8CC8HE+QRvgo8By4JEOPywDfhLW/BJQ12FdlwMrwsdHBqH209kf9IeF\nv0grwj+AinB+Zfh8Rbj8sA7v/2L4fSxlAL0r+qjxWKA+3L/3hL/8Rbtvga8DS4CFwG/C4CmafQv8\njuD8QQvBf0xXFHJ/AnXh974S+H90OZFegFpXELRht/2t3djXPqOHnOjp51LIerssX8P+oB+Ufash\nEEREEi4JbfQiItILBb2ISMIp6EVEEk5BLyKScAp6EZGEU9BLoplZq5kt6PCYG85/PBzJ8AUz+7uZ\nzQjnl5vZ9eHIgMvN7E9mNrnD+g40s9vNbKWZzTez+8zsSDOb1nW0QjP7mpn9Rzh9cjhC4gILRt/8\n2iDuBhnmSvt+iciQttfdj+1h2Qfcvd7M5hBcLHQR8G2C0UdnuHurmX0EuMvMTgrfczdwi7u/D8DM\nZhGMCbO+++o7uQW4xN1fMLMSYMbAvi2R3CnoReBvwGfCMUk+Ahzq7q0A7v4rM7ucYIgIB1rc/ca2\nN3o4SJV1GEO+BwcQXERDuO6XC/w9iPRIQS9JN8LMFnR4fq27/77La/4PwVWJ04F13mUwOoIrco8O\np3sbBO7wLts6kGAUS4AfAUvN7HHgAYL/Cvbl/m2I5E9BL0nXW9PNrWa2l+CS9E8SDKswECs7bqtj\nO7y7f8PMbiW4YcalBOOInz7A7YnkREEvw9kH3L2+7YmZbQWmmlm1u+/q8LrjCW4eAnBxvhtz95XA\nz8zsF0CDmU1w9y35rk8kV+p1IxJy9z0EJ01/GJ4wxcw+SDAK5WPhoyI8eUu4/Bgze0tf6zazCzrc\n2/MIoBXYXuBvQSQrBb0k3Ygu3Su/08frrwb2AcvMbDnwL8C7PERw27p3hN0rFwHXEtyNqS+XEbTR\nLyAYzfIDbSd8RaKm0StFRBJOR/QiIgmnoBcRSTgFvYhIwinoRUQSTkEvIpJwCnoRkYRT0IuIJNz/\nAlGlAAAy7OSHAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Ne2EmO44yobv","colab_type":"text"},"source":["### Change another type of RNN cell\n","We are using LSTM cell as the original work, but GRU cell is getting more popular today, let's chage the cell type layer to GRU cell and see how it performs. Your parameters should be the same as above to compare the two units"]},{"cell_type":"code","metadata":{"id":"AieHUA47yobw","colab_type":"code","colab":{}},"source":["# these are preset parameters, you can change them to get better result\n","display_step = 300\n","\n","epochs = 13\n","batch_size = 128\n","\n","rnn_size = 128\n","num_layers = 3\n","\n","encoding_embedding_size = 200\n","decoding_embedding_size = 200\n","\n","learning_rate = 0.001\n","keep_probability = 0.5\n","grad_clip = 5\n","cell_type = 'GRU' #THIS IS CHANGED TO GRU\n","rnn2_loss_history = []"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fr57ZU3Vyobx","colab_type":"code","outputId":"bbc1326a-7734-4faf-c32f-54cd11c8ba9e","executionInfo":{"status":"ok","timestamp":1574923663682,"user_tz":300,"elapsed":4399,"user":{"displayName":"Abhijeet Arunkumar Mishra","photoUrl":"","userId":"02580029324233760324"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["save_path = 'checkpoints/dev_GRU'\n","(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n","max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n","\n","train_graph = tf.Graph()\n","with train_graph.as_default():\n","    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n","    lr, keep_prob = hyperparam_inputs()\n","    \n","    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n","                                                   targets,\n","                                                   keep_prob,\n","                                                   batch_size,\n","                                                   target_sequence_length,\n","                                                   max_target_sequence_length,\n","                                                   len(source_vocab_to_int),\n","                                                   len(target_vocab_to_int),\n","                                                   encoding_embedding_size,\n","                                                   decoding_embedding_size,\n","                                                   rnn_size,\n","                                                   num_layers,\n","                                                   target_vocab_to_int,\n","                                                   cell_type)\n","    \n","    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n","    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n","\n","    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n","    # - Returns a mask tensor representing the first N positions of each cell.\n","    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n","\n","    with tf.name_scope(\"optimization\"):\n","        # Loss function - weighted softmax cross entropy\n","        cost = tf.contrib.seq2seq.sequence_loss(\n","            training_logits,\n","            targets,\n","            masks)\n","\n","        # Optimizer\n","        train_op = my_optimizer(cost,grad_clip,lr)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /gdrive/My Drive/DL-Assignment 3/utils/nmt.py:191: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sliZJ9B0yoby","colab_type":"code","outputId":"b745cf9b-1548-4bce-e51e-d12190ca0ed6","executionInfo":{"status":"ok","timestamp":1574925277598,"user_tz":300,"elapsed":1605498,"user":{"displayName":"Abhijeet Arunkumar Mishra","photoUrl":"","userId":"02580029324233760324"}},"colab":{"base_uri":"https://localhost:8080/","height":697}},"source":["\n","\n","# Split data to training and validation sets\n","train_source = source_int_text[batch_size:]\n","train_target = target_int_text[batch_size:]\n","valid_source = source_int_text[:batch_size]\n","valid_target = target_int_text[:batch_size]\n","(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n","                                                                                                             valid_target,\n","                                                                                                             batch_size,\n","                                                                                                             source_vocab_to_int['<PAD>'],\n","                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n","with tf.Session(graph=train_graph) as sess:\n","    sess.run(tf.global_variables_initializer())\n","\n","    for epoch_i in range(epochs):\n","        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n","                get_batches(train_source, train_target, batch_size,\n","                            source_vocab_to_int['<PAD>'],\n","                            target_vocab_to_int['<PAD>'])):\n","\n","            _, loss = sess.run(\n","                [train_op, cost],\n","                {input_data: source_batch,\n","                 targets: target_batch,\n","                 lr: learning_rate,\n","                 target_sequence_length: targets_lengths,\n","                 keep_prob: keep_probability})\n","            rnn2_loss_history.append(loss)\n","\n","\n","            if batch_i % display_step == 0 and batch_i > 0:\n","                batch_train_logits = sess.run(\n","                    inference_logits,\n","                    {input_data: source_batch,\n","                     target_sequence_length: targets_lengths,\n","                     keep_prob: 1.0})\n","\n","                batch_valid_logits = sess.run(\n","                    inference_logits,\n","                    {input_data: valid_sources_batch,\n","                     target_sequence_length: valid_targets_lengths,\n","                     keep_prob: 1.0})\n","\n","                train_acc = get_accuracy(target_batch, batch_train_logits)\n","                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n","\n","                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n","                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n","\n","    # Save Model\n","    saver = tf.train.Saver()\n","    saver.save(sess, save_path)\n","    print('Model Trained and Saved')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch   0 Batch  300/1077 - Train Accuracy: 0.5425, Validation Accuracy: 0.5312, Loss: 1.4696\n","Epoch   0 Batch  600/1077 - Train Accuracy: 0.6888, Validation Accuracy: 0.6962, Loss: 1.0733\n","Epoch   0 Batch  900/1077 - Train Accuracy: 0.7148, Validation Accuracy: 0.7036, Loss: 0.9202\n","Epoch   1 Batch  300/1077 - Train Accuracy: 0.7513, Validation Accuracy: 0.7274, Loss: 0.7419\n","Epoch   1 Batch  600/1077 - Train Accuracy: 0.7496, Validation Accuracy: 0.7409, Loss: 0.6849\n","Epoch   1 Batch  900/1077 - Train Accuracy: 0.7509, Validation Accuracy: 0.7513, Loss: 0.6777\n","Epoch   2 Batch  300/1077 - Train Accuracy: 0.7617, Validation Accuracy: 0.7535, Loss: 0.6235\n","Epoch   2 Batch  600/1077 - Train Accuracy: 0.7691, Validation Accuracy: 0.7617, Loss: 0.5891\n","Epoch   2 Batch  900/1077 - Train Accuracy: 0.7622, Validation Accuracy: 0.7652, Loss: 0.5997\n","Epoch   3 Batch  300/1077 - Train Accuracy: 0.7847, Validation Accuracy: 0.7739, Loss: 0.5662\n","Epoch   3 Batch  600/1077 - Train Accuracy: 0.7782, Validation Accuracy: 0.7743, Loss: 0.5419\n","Epoch   3 Batch  900/1077 - Train Accuracy: 0.7912, Validation Accuracy: 0.7812, Loss: 0.5289\n","Epoch   4 Batch  300/1077 - Train Accuracy: 0.8129, Validation Accuracy: 0.8047, Loss: 0.4873\n","Epoch   4 Batch  600/1077 - Train Accuracy: 0.8155, Validation Accuracy: 0.8116, Loss: 0.4617\n","Epoch   4 Batch  900/1077 - Train Accuracy: 0.8286, Validation Accuracy: 0.8173, Loss: 0.4441\n","Epoch   5 Batch  300/1077 - Train Accuracy: 0.8429, Validation Accuracy: 0.8368, Loss: 0.4158\n","Epoch   5 Batch  600/1077 - Train Accuracy: 0.8620, Validation Accuracy: 0.8624, Loss: 0.3729\n","Epoch   5 Batch  900/1077 - Train Accuracy: 0.8646, Validation Accuracy: 0.8668, Loss: 0.3686\n","Epoch   6 Batch  300/1077 - Train Accuracy: 0.8767, Validation Accuracy: 0.8724, Loss: 0.3348\n","Epoch   6 Batch  600/1077 - Train Accuracy: 0.8811, Validation Accuracy: 0.8811, Loss: 0.3191\n","Epoch   6 Batch  900/1077 - Train Accuracy: 0.8832, Validation Accuracy: 0.8880, Loss: 0.3182\n","Epoch   7 Batch  300/1077 - Train Accuracy: 0.8963, Validation Accuracy: 0.8898, Loss: 0.2917\n","Epoch   7 Batch  600/1077 - Train Accuracy: 0.8924, Validation Accuracy: 0.8967, Loss: 0.2949\n","Epoch   7 Batch  900/1077 - Train Accuracy: 0.8971, Validation Accuracy: 0.8924, Loss: 0.2853\n","Epoch   8 Batch  300/1077 - Train Accuracy: 0.9036, Validation Accuracy: 0.9010, Loss: 0.2668\n","Epoch   8 Batch  600/1077 - Train Accuracy: 0.9128, Validation Accuracy: 0.9049, Loss: 0.2469\n","Epoch   8 Batch  900/1077 - Train Accuracy: 0.9071, Validation Accuracy: 0.9089, Loss: 0.2522\n","Epoch   9 Batch  300/1077 - Train Accuracy: 0.9145, Validation Accuracy: 0.9097, Loss: 0.2356\n","Epoch   9 Batch  600/1077 - Train Accuracy: 0.9184, Validation Accuracy: 0.9158, Loss: 0.2288\n","Epoch   9 Batch  900/1077 - Train Accuracy: 0.9206, Validation Accuracy: 0.9149, Loss: 0.2361\n","Epoch  10 Batch  300/1077 - Train Accuracy: 0.9249, Validation Accuracy: 0.9184, Loss: 0.2155\n","Epoch  10 Batch  600/1077 - Train Accuracy: 0.9297, Validation Accuracy: 0.9180, Loss: 0.2071\n","Epoch  10 Batch  900/1077 - Train Accuracy: 0.9249, Validation Accuracy: 0.9227, Loss: 0.2285\n","Epoch  11 Batch  300/1077 - Train Accuracy: 0.9297, Validation Accuracy: 0.9193, Loss: 0.2089\n","Epoch  11 Batch  600/1077 - Train Accuracy: 0.9319, Validation Accuracy: 0.9197, Loss: 0.2001\n","Epoch  11 Batch  900/1077 - Train Accuracy: 0.9262, Validation Accuracy: 0.9253, Loss: 0.1898\n","Epoch  12 Batch  300/1077 - Train Accuracy: 0.9379, Validation Accuracy: 0.9319, Loss: 0.1495\n","Epoch  12 Batch  600/1077 - Train Accuracy: 0.9453, Validation Accuracy: 0.9336, Loss: 0.1418\n","Epoch  12 Batch  900/1077 - Train Accuracy: 0.9423, Validation Accuracy: 0.9358, Loss: 0.1544\n","Model Trained and Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cMNipW-6yob0","colab_type":"code","colab":{}},"source":["# Save parameters for checkpoint\n","save_params(save_path,cell_type)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RwrsIqj7yob1","colab_type":"code","outputId":"99907f8c-f201-4076-e078-ef38faaaab06","executionInfo":{"status":"ok","timestamp":1574925449354,"user_tz":300,"elapsed":772,"user":{"displayName":"Abhijeet Arunkumar Mishra","photoUrl":"","userId":"02580029324233760324"}},"colab":{"base_uri":"https://localhost:8080/","height":279}},"source":["# plot loss history\n","# plot loss history\n","loss_data = rnn2_loss_history\n","from matplotlib import pyplot as plt\n","plt.plot(np.arange(len(loss_data)),loss_data)\n","plt.ylabel('Training_loss')\n","plt.xlabel('EPOCHS')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAEGCAYAAABrQF4qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAfKUlEQVR4nO3deXxddZ3/8dcnN0vbNLQpDaV0oQst\nyNZSMoKyVQShhSmj4yjLqKOOHUVH+ekMU2D8DTguqD8VUBQ7iqMzCG51mGEdFisuCE2hlLa0dAdK\nl3Rv0ma7+fz+uCfpTXLS3Cb33OXk/Xw87iP3nnNzzicn6bvf+z3f8z3m7oiISHyV5LsAERGJloJe\nRCTmFPQiIjGnoBcRiTkFvYhIzJXmu4B0o0eP9kmTJuW7DBGRorJ06dKd7l7T2/qCCvpJkyZRV1eX\n7zJERIqKmW0+0np13YiIxJyCXkQk5hT0IiIxp6AXEYk5Bb2ISMwp6EVEYk5BLyISc7EI+qbWJN/8\n3zUs3bw736WIiBScWAR9a7Kdu55exwub9+a7FBGRghOLoDczABzdREVEpLt4BH3wVTfLEhHpKR5B\nHyS9cl5EpKd4BH3QpleLXkSkp3gEfWeLXkkvItJdLIK+g1r0IiI9xSLoO1r0IiLSUzyCvrOPXk16\nEZHuYhH0JR199Mp5EZEeYhH0HRdMtSvoRUR6iEfQB1816kZEpKd4BL26bkREehWToO+Y60ZERLqL\nRdB3UpNeRKSH0qh3YGabgANAEmhz99po9qMWvYhImMiDPvAOd98Z5Q4MNehFRMLEpuvGzDTqRkQk\nRC6C3oH/NbOlZja/+0ozm29mdWZWV19f3++dqEUvIhIuF0F/vrvPAuYAnzSzC9NXuvtCd69199qa\nmpp+70R99CIi4SIPenffEnzdAfwaeGsU+zFMLXoRkRCRBr2ZVZpZVcdz4F3Aimh2pitjRUTCRD3q\nZgzw6+CCplLgp+7+WBQ7MlDfjYhIiEiD3t03ADOi3EcH9dGLiISLz/BKTPPRi4iEiE/Qm4ZXioiE\niU/Qo64bEZEw8Ql60/BKEZEw8Ql6NLxSRCRMfIJeffQiIqFiFPQadSMiEiZGQa+TsSIiYeIT9Kjr\nRkQkTHyCXvPRi4iEik/Qoxa9iEiY+AS9+uhFRELFJujRfPQiIqFiE/SmeYpFRELFJ+hRH72ISJj4\nBL2ujBURCRWfoEfDK0VEwsQn6NWiFxEJFZ+gR6diRUTCxCfoNR+9iEio2AQ9aD56EZEwsQl6U9+N\niEioWAW9cl5EpKf4BD268YiISJj4BL1a9CIioWIT9CUadSMiEio2QW9Au5JeRKSH2AQ96roREQmV\nk6A3s4SZvWhmD0W2D1DSi4iEyFWL/jPAK1HuQPeMFREJF3nQm9l44ArgB1Hup8SgvT3KPYiIFKdc\ntOjvAG4EQmPYzOabWZ2Z1dXX1/d7J4mSEpI6GSsi0kOkQW9mVwI73H1pb+9x94XuXuvutTU1Nf3e\nV6IEku0KehGR7qJu0Z8HzDOzTcADwMVm9p9R7ChhpqAXEQkRadC7+03uPt7dJwFXA0+7+19Hsa+S\nEtM4ehGRELEZR68WvYhIuNJc7cjdFwOLo9p+okRBLyISJj4tegW9iEioeAW9+uhFRHqITdCXmNGu\nFr2ISA+xCXq16EVEwsUr6DUFgohID/EJejOSmuxGRKSH+AS9Rt2IiISKTdCnrozNdxUiIoUno6A3\ns6+Y2TFmVmpmj5vZdjO7NurijkbCNKmZiEiYTFv0c9x9P3Al8CZwCvBPkVXVD4mSEgW9iEiITIO+\nY6qEucAv3H0PBXbjPk1TLCISLtO5bh41sxVAEvikmY0GmqMr6+hpHL2ISLiMWvTu/o/AxcDZ7t4K\nHALeE2VhR0tXxoqIhMv0ZOx7gEPu3mZmC4AfAf2/HVQESkuMNgW9iEgPmfbR3+ruB8zs7aT66e8D\n7omurKOXKClRi15EJESmQZ8Mvl4JfN/dHwQqoimpf0oTatGLiITJ9GTsVjO7G5gDnG1m5RTYxVa6\nMlZEJFymYf0+4LfA3GBo5WhgQWRV9UOqj15z3YiIdJfpqJsGYCUw28w+DlS7+6ORVnaUEsEUCOqn\nFxHpKtNRN58CfgFMDB4/N7ProyzsaJWWGIDG0ouIdJNpH/184K1Byx4z+zLwR+C7URV2tBIlqf+z\nku1OWSLPxYiIFJBM++gNaEl73RosKxgdLXqNvBER6SrTFv1/AM+Z2a+C1+8GfhxNSf2T6Oi6SSro\nRUTSZRT07v41M1sMnB8s+ri7L4msqn4oTXS06DXyRkQk3RGD3syOSXu5Onh0rgumLi4InS16dd2I\niHTRV4t+JanpiDv64ztS1ILnEyOq66ipj15EJNwRg97dJ2SyETM7xd1X9/3O6KSPuhERkcOyNY3B\nT8MWmtkQM3vezF4ys5VmdluW9tfDgaZWAF54bU9UuxARKUrZCvrehlo2Axe7+wxgJnC5mZ2bpX12\nsWTTbgC+/fS6KDYvIlK0Mh1e2ZfQ/hJ3d6AheFkWPCLpWznQ1AbAxp2NUWxeRKRoRT4DpZklzGwZ\nsAN4wt2fi2I/B1tSMymrj15EpKtsBX2ytxXunnT3mcB44K1mdnr6ejObb2Z1ZlZXX1/f7wIagha9\niIh0lVHXjZmdGbJ4H/C6u7e7+5/1tQ1332tmvwEuB1akLV8ILASora3td3O842SsiIh0lWmL/ofA\nUuAnpKZDqAMeBNaa2Tt7+yYzqzGzkcHzocClpF10lU2fv/LUKDYrIlL0Mg36TcDZ7j4zGEFzNvAq\ncBnwjSN831jgN2a2HFhCqo/+oQHU26u3Tx0dxWZFRIpepqNu3uLuyzteuPvLZnaqu68z630Sy+B7\nzhpgjRnRHDciIuEyDfrVZvZt4IHg9fuDZRVAQZwFHVVZnu8SREQKUqZdNx8E3iB1n9gFwJvAh0iF\nfK999Ll0pE8WIiKDWabTFB8Evho8utuX1YpERCSrMh1eeS7wL8CJ6d/j7tMjqktERLIk0z76HwE3\nkhpi2evFUSIiUngyDfr97v4/kVYiIiKRyDTonzazrwCLSM1ICXQOnxQRkQKWadCf3+0rpGahvDC7\n5WRHW7Kd0kTk87WJiBSFTEfdXBB1Idn00ht7OfvEUfkuQ0SkIPR1c/Br3P1+M/t02Hp3vyuasgbm\nL7/3LJtuvyLfZYiIFIS+WvTVwdeaqAsREZFo9HVz8O8GXz+fm3JERCTbMr1gajTwEWASXS+Ymh9N\nWf3znrPGsejFLfkuQ0SkoGQ66uZB4E/A7yngC6ZqqiryXYKISMHJNOgr3f1zkVYiIiKRyHSw+aNm\n9q5IK8mC8dVD812CiEjByTToPw48ZmYNZrbbzPaY2e4oC+uP86dpcJCISHeZdt3oPn0iIkWqrwum\nprn7WuC0Xt5SsHPdJNudRIluRiIi0leLfgHwUeDukHUFN9fNmGMOj7rZf6iVat1eUESkzwumPhp8\nLYq5boaVH/5xmtuiuVl4/YFmhpYnGF6Raa+XiEh+ZTzFo5mdYmbvMbNrOx5RFtZf76+dAMB/LYvm\nwqk/+9KTXPrN30aybRGRKGQU9Gb2z8BC4B5gDnAH8N4I6+q3zbsbAbj90dWR7WPrvqbIti0ikm2Z\ntujfD7wD2OruHwBmAJWRVTUAUXXZiIgUq0yD/pC7J4E2M6sCtpG6UXjBefG1vfkuQUSkoGQa9C+a\n2UjgXqAOeD54FJyLpuuiKRGRdH0GvZkZcKu773X3u4ErgL9z9w9GXl0/3DqvtyH/IiKDU59jBN3d\nzewJ4PTg9brIqxqAEUPL8l2CiEhBybTrZpmZnXW0GzezCWb2GzNbZWYrzewzR7uNozVKF0mJiHTR\n1xQIpe7eBpwFLDGz9UAjYKQa+7P62H4b8Dl3fyE4ibvUzJ5w91XZKF5ERPrWV9fN88AsYF5/Nu7u\nW4GtwfMDZvYKMA6INOin1FSyob4xyl2IiBSNvoLeANx9/UB3ZGaTSH0yeG6g2+pLR8hrYjMRkb6D\nvsbMPtvbSnf/ZiY7MbPhwK+AG9x9f7d184H5ABMnTsxkcxl7aPmbXDVzXFa3KSJSbPo6GZsAhgNV\nvTz6ZGZlpEL+Pndf1H29uy9091p3r62pye4Y+MVr6rO6PRGRYtRXi36ru3+hvxsPxuD/EHgl09Z/\nNj3y8la+9f6Zud6tiEhB6atFP9AO7vOADwAXm9my4DF3gNvMmOa9ERHpu0X/zoFs3N1/z8D/sxAR\nkQE4Yove3QvuBuCZ+Jc/PzXfJYiIFIyMbzxSTK44c2y+SxARKRixDPrqYYenQWhuS+axEhGR/Itl\n0Cfs8GmBh17amsdKRETyL5ZBX5J2NeznfvFSHisREcm/WAa9iIgcpqAXEYk5Bb2ISMzFNuhnTRzZ\n+fxgS1seKxERya/YBv0tVxy+aOqrj67OYyUiIvkV26A/+8TqzudPrd6Rx0pERPIrtkGf7o09h/Jd\ngohI3gyKoBcRGcwGTdBv2ql7yIrI4DRogn59fUO+SxARyYtBE/Q/+N3GfJcgIpIXsQ76l299V+fz\nZzfsor3dWbp5D9fft5T2ds9jZSIiudPXHaaKWtWQsi6vv/jwKzy4bAu7GlvYNa+FmqqKPFUmIpI7\nsW7Rd3fvHzbS0Y4v0Q0ORWSQiH3QD6/o+qHFPRX1Zkp6ERkcYh/0Jx9f1eX1noOtwOHAFxGJu9gH\n/Q8/VBu6vE0nY0VkkIh90I9Mu39sukdf1i0GRWRwiH3QA2y6/YoeyxpbdNNwERkcBkXQh/n642vy\nXYKISE4M2qAHSKqfXkQGgUET9GHdN1NvfkRhLyKxN2iCvjdTb34k3yWIiEQq0qA3s3vNbIeZrYhy\nP5laedtlVFX0nPVhyabdeahGRCQ3om7R/ztwecT7yFhlRSnL0yY66/BX9zzLx35Sl4eKRESiF2nQ\nu/szQEE1l82ML1x1Wo/lT6zanodqRESiNyj76D9w7on5LkFEJGfyHvRmNt/M6sysrr6+Plf75IZL\npvVYvuBXy3OyfxGRXMp70Lv7QnevdffampqanO33hkum9xhy+cCS17n4G4v50sOrmLTg4ZzVIiIS\npbwHfb6t/dKcLq831Dfyb8FtB3/63Gv5KElEJKuiHl55P/AscLKZvWFmH41yf/1Rlijhe9fNCl13\n869f7vX72pLtUZUkIpJVkd5K0N2viXL72TLnjLG9rmtqTTKkLNFjed3mPZw75dgoyxIRyYpB33XT\nYfE/zA5dfsrnH2PdjoYeNxMvS+jQiUhxsEK601Jtba3X1eX3wqVMT8JOHzOcxz5zISW6+ayI5JmZ\nLXX38LssoRZ9D5tuv4K/u3BKn+97dXsDU25+hEkLHmZ9fYNuTSgiBUst+iPozxDL95w1jkUvbgHg\nunMm8tlLp7P8jX2cPm4ENVUVXd7bmmxXF5CIDFhfLXoFfR9+tuQ1/ulXvY++ORrHVVUwpaaSP204\nPCvE354/mX++8tSsbF9EBicFfZb890tv8un7XwTgzqtn8pkHlkW2r+dufidjjhnS+fqxFdv4+H8u\nZcVtlzE8ZPZNERnc+gp6pUaG5s04gXkzTuh8fdH0Grbua2J4RSkXfO03Wd3XOV9+KvV18ii+9t4z\nufOptQCsenM/tSdW09bulJemunxak+3sP9TKscMret2eiAxuatFn0fr6BqbWDOe7i9dRPayce3+/\nkbU7GnJaw9CyBJ+6+CRGDivjwmk1VFeWs2N/E2WJEp5evYO/PvdEEhopJBIr6ropIAeaWvnu4vU0\nNLXx6Iqt7GxoyUsdp51wDCvf3M+DnzyPGRNG9msbLW3tLHxmPR+7cAoVpT0vKBOR3FHQF4nG5jau\nuvsPrMvxJ4AwHzlvMjVVFXz1sdUA3DL3LVxy6hjWbj/ApaeOYcmmPSxYtJwN9Y3cePnJXD/7pDxX\nLDK4KeiLWFNrkvX1Dfxu7U7GHFPBz5a83mXETiG54ZJp/OWs8UwYNYym1iQNzW2M1nkDkZxQ0Mdc\nst3Z1dhMVUUZQ8tTXSg7G5o51JKkoqyEbz+1jtpJ1ZGOEkp31sSRrNl2gIMtSVbedhnb9zcxeXQl\nZsauhmbanc7rCZ7fuJtjh5cztWZ4TmoTiSsFvfTqtV0H+cP6ndy0KDvXCWRq3MihDK8oZc32AwA9\n7gsgIkdHQS9Z09LWzpJNu1myaTdNre1cOH00i9fUs/CZDVnbx8cumMyn3jGNYRWJzquGm1qTvL77\nINPGVB3xe/+4fiezJlb3mG20pa0dM01EJ/GloJeceH33QUpKjPNufzryfZUYPH7DhVQGnwrecfJx\nrNl2gMvueIbrzpnIl959Rpf3T77pYcZUDeFPN78z8tpE8kEXTElOTBg1DOjZDdPUmuTVoItm36FW\njq2sYO5dvxvQvtodLv3WM6Hr7nvuNf71qtMxg5ZkOxWlCdxh2/6mAe0zTEcjyUzXJRSLdTsOcGxl\nBdWV5fkuJafUope8aWpNcqCpjZqqCh5fuY1P3/8izW3R3rnrt/84m4mjhnUJ5/Z2508bdvH2k0Yf\n1bYmLXiYK88cy3euDb9DmRSeSQse5oQRQ/jjTfH6dKcWvRSsIWWJzv70y047njVfnBP6vidXbWfs\nyCEMLUtw/X0vUH+gmV2N/bvY7KKvLz6q99eeWM1F02u47twTcXd2HGhm8uhKNtQ3AvDQ8q1859oj\nb2P7/iZGDivr88Kygy1ttDuazyhib+7L/qe7QqcWvRS95rYk31u8njueXJvvUqiqKOVAcxuzT66h\n3aGyPMGjK7bxzlOO4xOzpzJyWDnDyhNUDSllfX0ji154g49dMIUJo4Yx+aaHcdcopCh1TD0et2Os\nk7EyKLk7i17Ywp6DLZw+bgSjKst5Vy/9+oXqbVOO5dkNuzpfv/uscQyvKOXNvYe4ae4pJNthz8EW\nZk4YSXmihL2HWlm8ZgePvLyVJ1/ZwZ1Xz+SqmePY1dDMMUPLKEuU8KcNu7jxl8v5/gfO5i1jj8nj\nT5cfCvoCoKCXXGtobqP+QDMTqofS1u4sf2MfjS1t3LN4PXsOtvDq9tSUFFNGV7JhZ2Oeq43Wv/7F\n6Wzde4hEiVFTVcHp40YwrDzBlj2HuGBaDb9fV09LWztTaobz4mt7aE06F02v4fU9B3n71NG0tzt/\nWL+TtqQz++QamtvaSbY7dZv3cMFJo9nZ0MxxadNvh9l3qBV3Z+SwaE6WdgT9+i/PjdXkfgp6kQhs\n2XsISF38BakTyw8u28Kw8lL2N7Xy/d9u4JPvmJq1m9YMZmNHDGFrWr/6vBkncPaJ1fz7HzexMfjP\nt+PTzydmT6Vu025mjB/JNedM5NjKcr7z9Dp+8PuNPbZbNaSUA01trLjtMtqS7cy583ecM3kU33r/\nzKIbSaWgFylwyXZn485GTjouNRVEY3MbX37kFT5+0VSOGVLG46u2ceMvl3f5ngmjhvL67kP5KHdQ\nOHP8CK5960TGVw/jvJOOZd+h1sg+ZWSDgl5EaG5L0t4OiRLjiVXbGVc9lANNrbyweS/fevLVfJdX\nlObNOIEvXHUadZv2MKwiwayJ1ZSWGKWJEhqb29jd2NJ5fUl37k7tF59kzhnH88W/OCP0PUdDQS8i\nkXP30O6OQy1JmlqTlJeW8PDyrRwztJSZE6q57X9W8uiKbdw89xSWbt7DvkOtnTOzjqosZ3c/h89m\n4pa5b+FLj7wS2faP5IxxI3h5y74uy6bUVDJvxgm8r3YCJwRdgUdLQS8ig8KuhmYqyhJUlJbw7Ppd\nnDFuRJcrYJvbkn1ey7ChvoHNuw/y4R8tibrcUP0dDaQLpkRkUEi/b/KF02t6rM/kTmhTaoYzpWZ4\nZ+A2tSYpLTF2H2zhhc17mTVxJBd/47c0NLdF/skjmxT0IiK96Lhy+7iqIVx++vEArLjtsh7v6951\n5e5s29/EqMpydja08Mqb+xlWkWBoWYJpY6p4fMU2Zk4cyQkjhnLHU6/y2Ipt3HX1WZH9HJF33ZjZ\n5cCdQAL4gbvf3tt71XUjInL0+uq6iXSCbjNLAHcDc4BTgWvM7NQo9ykiIl1FfSeGtwLr3H2Du7cA\nDwBXRbxPERFJE3XQjwNeT3v9RrBMRERyJO/3VjOz+WZWZ2Z19fX1+S5HRCR2og76LcCEtNfjg2Wd\n3H2hu9e6e21NTc8hUSIiMjBRB/0SYJqZTTazcuBq4L8j3qeIiKSJdBy9u7eZ2aeAx0kNr7zX3VdG\nuU8REekq8gum3P0R4JGo9yMiIuEKaq4bM6sHNg9gE6OBnVkqJ2rFVCsUV73FVCsUV73FVCsUV70D\nqfVEd+/1JGdBBf1AmVndka4OKyTFVCsUV73FVCsUV73FVCsUV71R1pr34ZUiIhItBb2ISMzFLegX\n5ruAo1BMtUJx1VtMtUJx1VtMtUJx1RtZrbHqoxcRkZ7i1qIXEZFuFPQiIjEXi6A3s8vNbI2ZrTOz\nBXmqYYKZ/cbMVpnZSjP7TLB8lJk9YWZrg6/VwXIzs7uCmpeb2ay0bX0oeP9aM/tQxHUnzOxFM3so\neD3ZzJ4L6vpZMHUFZlYRvF4XrJ+Uto2bguVrzKzn7XeyU+dIM/ulma02s1fM7G2FfGzN7P8Efwcr\nzOx+MxtSSMfWzO41sx1mtiJtWdaOp5mdbWYvB99zl1nIncMHVuvXg7+F5Wb2azMbmbYu9Jj1lhO9\n/V6yWW/aus+ZmZvZ6OB1bo6tuxf1g9TUCuuBKUA58BJwah7qGAvMCp5XAa+SutnK14AFwfIFwFeD\n53OBRwEDzgWeC5aPAjYEX6uD59UR1v1Z4KfAQ8HrnwNXB8/vAT4RPL8euCd4fjXws+D5qcExrwAm\nB7+LRAR1/hj42+B5OTCyUI8tqam4NwJD047p3xTSsQUuBGYBK9KWZe14As8H77Xge+dkudZ3AaXB\n86+m1Rp6zDhCTvT2e8lmvcHyCaSmg9kMjM7lsY0kPHL5AN4GPJ72+ibgpgKo60HgUmANMDZYNhZY\nEzz/PnBN2vvXBOuvAb6ftrzL+7Jc43jgKeBi4KHgD2dn2j+gzmMb/IG+LXheGrzPuh/v9Pdlsc4R\npILTui0vyGPL4fswjAqO1UPAZYV2bIFJdA3PrBzPYN3qtOVd3peNWrutezdwX/A89JjRS04c6W8+\n2/UCvwRmAJs4HPQ5ObZx6LopuJubBB+9zwKeA8a4+9Zg1TZgTPC8t7pz+fPcAdwItAevjwX2untb\nyL476wrW7wven4t6JwP1wI8s1c30AzOrpECPrbtvAf4f8BqwldSxWkphHtt02Tqe44Ln3ZdH5SOk\nWrb0UVPY8iP9zWeNmV0FbHH3l7qtysmxjUPQFxQzGw78CrjB3fenr/PUf8EFMZ7VzK4Edrj70nzX\nkoFSUh+Fv+fuZwGNpLoWOhXYsa0mdcvMycAJQCVweV6LOkqFdDyPxMxuAdqA+/JdS2/MbBhwM/B/\n81VDHIK+z5ub5IqZlZEK+fvcfVGweLuZjQ3WjwV2BMt7qztXP895wDwz20TqXr4XA3cCI82sY1bT\n9H131hWsHwHsylG9bwBvuPtzwetfkgr+Qj22lwAb3b3e3VuBRaSOdyEe23TZOp5bgufdl2eVmf0N\ncCVwXfAfU39q3UXvv5dsmUrqP/2Xgn9v44EXzOz4ftTbv2Obrf6+fD1ItfY2BAey4yTLaXmow4Cf\nAHd0W/51up7g+lrw/Aq6noR5Plg+ilR/dHXw2AiMirj22Rw+GfsLup6Yuj54/km6njD8efD8NLqe\n/NpANCdjfwecHDy/NTiuBXlsgXOAlcCwoIYfA39faMeWnn30WTue9DxhODfLtV4OrAJqur0v9Jhx\nhJzo7feSzXq7rdvE4T76nBzbyMIjlw9SZ65fJXVW/ZY81XA+qY+6y4FlwWMuqT7Ap4C1wJNpvywD\n7g5qfhmoTdvWR4B1wePDOah9NoeDfkrwh7Qu+AdQESwfErxeF6yfkvb9twQ/xxoGMLqijxpnAnXB\n8f2v4I+/YI8tcBuwGlgB/EcQPAVzbIH7SZ0/aCX1iemj2TyeQG3ws68HvkO3E+lZqHUdqT7sjn9r\n9/R1zOglJ3r7vWSz3m7rN3E46HNybDUFgohIzMWhj15ERI5AQS8iEnMKehGRmFPQi4jEnIJeRCTm\nFPQSa2aWNLNlaY8FwfLFwUyGL5nZH8zs5GB5uZndEcwMuNbMHjSz8WnbO97MHjCz9Wa21MweMbPp\nZjap+2yFZnarmf1D8PzcYIbEZZaaffPWHB4GGeRK+36LSFE75O4ze1l3nbvXmdl8UhcLzQO+TGr2\n0ZPdPWlmHwYWmdk5wff8Gvixu18NYGYzSM0J83rPzXfxY+B97v6SmSWAkwf2Y4lkTkEvAs8ANwRz\nknwYmOzuSQB3/5GZfYTUFBEOtLr7PR3f6MEkVZY2h3wvjiN1EQ3Btldl+WcQ6ZWCXuJuqJktS3v9\nFXf/Wbf3/DmpqxJPAl7zbpPRkboi97Tg+ZEmgZvabV/Hk5rFEuBbwBozWww8RupTQVPmP4ZI/yno\nJe6O1HVzn5kdInVJ+t+TmlZhINan7yu9H97dv2Bm95G6Yca1pOYRnz3A/YlkREEvg9l17l7X8cLM\ndgMTzazK3Q+kve9sUjcPAXhvf3fm7uuB75nZvwH1Znasu+/q7/ZEMqVRNyIBd28kddL0m8EJU8zs\ng6RmoXw6eFQEJ28J1p9pZhf0tW0zuyLt3p7TgCSwN8s/gkgoBb3E3dBuwytv7+P9NwFNwKtmthb4\nK+DdHiB127pLguGVK4GvkLobU18+QKqPfhmp2Syv6zjhKxI1zV4pIhJzatGLiMScgl5EJOYU9CIi\nMaegFxGJOQW9iEjMKehFRGJOQS8iEnP/H91/BQ7NBZKJAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"2mM--7Hlyob3","colab_type":"text"},"source":["## Translate\n","This will translate translate_sentence from French to English"]},{"cell_type":"code","metadata":{"id":"h9-ryTHYyob4","colab_type":"code","colab":{}},"source":["cell_type = \"LSTM\" #change this to LSTM or GRU to traslate using appropriate model\n","_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n","load_path = load_params(cell_type)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmjbS8Y6yob7","colab_type":"code","outputId":"6a72abbc-3106-44b5-88ff-e3f81ac50a04","executionInfo":{"status":"ok","timestamp":1574925459368,"user_tz":300,"elapsed":1913,"user":{"displayName":"Abhijeet Arunkumar Mishra","photoUrl":"","userId":"02580029324233760324"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["\n","translate_sentence = side_by_side_sentences[0][1]\n","correct_translation = side_by_side_sentences[0][0]\n","\n","translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n","\n","loaded_graph = tf.Graph()\n","with tf.Session(graph=loaded_graph) as sess:\n","    # Load saved model\n","    loader = tf.train.import_meta_graph(load_path + '.meta')\n","    loader.restore(sess, load_path)\n","\n","    input_data = loaded_graph.get_tensor_by_name('input:0')\n","    logits = loaded_graph.get_tensor_by_name('predictions:0')\n","    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n","    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n","\n","    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n","                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n","                                         keep_prob: 1.0})[0]\n","\n","print('Input')\n","print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n","print('  French Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n","\n","print('\\nPrediction')\n","print('  Word Ids:      {}'.format([i for i in translate_logits]))\n","print('  English Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))\n","\n","print('\\nCorrect translation')\n","print('  English Words: {}'.format(correct_translation))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from checkpoints/dev_LSTM\n","Input\n","  Word Ids:      [297, 224, 128, 106, 229, 239, 245, 100, 223, 176, 301, 128, 347, 298, 49, 145]\n","  French Words: ['new', 'jersey', 'est', 'parfois', 'calme', 'pendant', \"l'\", 'automne', ',', 'et', 'il', 'est', 'neigeux', 'en', 'avril', '.']\n","\n","Prediction\n","  Word Ids:      [36, 68, 81, 144, 50, 86, 122, 65, 9, 39, 81, 50, 229, 198, 123, 1]\n","  English Words: new jersey is sometimes snowy during fall , and it is snowy in april . <EOS>\n","\n","Correct translation\n","  English Words: new jersey is sometimes quiet during autumn , and it is snowy in april .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"agiFh44Zyob9","colab_type":"text"},"source":["#### Compare your result and list pros and cons of using GRU cells compared to LSTM"]},{"cell_type":"markdown","metadata":{"id":"N2IiGfaWyob9","colab_type":"text"},"source":["Answer:\n","**\n","\n","PROS:\n","\n","1) GRU Cells are simpler as compared to LSTM cells\n","\n","2) GRU Cells are easier to train and less mathematically complicated. \n","\n","3) GRUs train faster and with lesser data.\n","\n","CONS:\n","\n","1) LSTM can store more data and thus can hold data over long time values\n","\n","2) Since, there are more werights, we have more control and thus flexibility over the output and data storage of a model in LSTM\n","\n","\n","Overall, GRUs are new, but are better in terms of computation, and provide almost the same, or better performance as LSTMs\n","**"]},{"cell_type":"code","metadata":{"id":"gigFLIZfDgyO","colab_type":"code","colab":{}},"source":["\n"],"execution_count":0,"outputs":[]}]}