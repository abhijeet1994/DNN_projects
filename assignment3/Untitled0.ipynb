{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cP-Er12j3_mo","colab_type":"code","colab":{}},"source":["# this file contains all the necessary data for the task 2 Neural machine translations\n","\n","import os\n","import pickle\n","import copy\n","import numpy as np\n","import time\n","import tensorflow as tf\n","import pickle\n","\n","\n","def load_data(path):\n","    input_file = os.path.join(path)\n","    with open(input_file, 'r', encoding='utf-8') as f:\n","        data = f.read()\n","\n","    return data\n","\n","\n","CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n","\n","def create_lookup_tables(text):\n","    # make a list of unique words\n","    vocab = set(text.split())\n","\n","    # starts with the special tokens\n","    vocab_to_int = copy.copy(CODES)\n","\n","    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n","    # since vocab_to_int already contains special tokens\n","    for v_i, v in enumerate(vocab, len(CODES)):\n","        vocab_to_int[v] = v_i\n","\n","    # (2)\n","    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n","\n","    return vocab_to_int, int_to_vocab\n","\n","def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n","    \"\"\"\n","        source_text, target_text: raw string text to be converted\n","        source_vocab_to_int, target_vocab_to_int: lookup tables for 1st and 2nd args respectively\n","    \n","        return: A tuple of lists (source_id_text, target_id_text) converted\n","    \"\"\"\n","    # empty list of converted sentences\n","    source_text_id = []\n","    target_text_id = []\n","    \n","    # make a list of sentences (extraction)\n","    source_sentences = source_text.split(\"\\n\")\n","    target_sentences = target_text.split(\"\\n\")\n","    \n","    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n","    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n","    \n","    # iterating through each sentences (# of sentences in source&target is the same)\n","    for i in range(len(source_sentences)):\n","        # extract sentences one by one\n","        source_sentence = source_sentences[i]\n","        target_sentence = target_sentences[i]\n","        \n","        # make a list of tokens/words (extraction) from the chosen sentence\n","        source_tokens = source_sentence.split(\" \")\n","        target_tokens = target_sentence.split(\" \")\n","        \n","        # empty list of converted words to index in the chosen sentence\n","        source_token_id = []\n","        target_token_id = []\n","        \n","        for index, token in enumerate(source_tokens):\n","            if (token != \"\"):\n","                source_token_id.append(source_vocab_to_int[token])\n","        \n","        for index, token in enumerate(target_tokens):\n","            if (token != \"\"):\n","                target_token_id.append(target_vocab_to_int[token])\n","                \n","        # put <EOS> token at the end of the chosen target sentence\n","        # this token suggests when to stop creating a sequence\n","        target_token_id.append(target_vocab_to_int['<EOS>'])\n","            \n","        # add each converted sentences in the final list\n","        source_text_id.append(source_token_id)\n","        target_text_id.append(target_token_id)\n","    \n","    return source_text_id, target_text_id\n","\n","def preprocess_and_save_data(source_path, target_path, text_to_ids):\n","    # Preprocess\n","    \n","    # load original data (English, French)\n","    source_text = load_data(source_path)\n","    target_text = load_data(target_path)\n","\n","    # to the lower case\n","    source_text = source_text.lower()\n","    target_text = target_text.lower()\n","\n","    # create lookup tables for English and French data\n","    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n","    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n","\n","    # create list of sentences whose words are represented in index\n","    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n","\n","    # Save data for later use\n","    pickle.dump((\n","        (source_text, target_text),\n","        (source_vocab_to_int, target_vocab_to_int),\n","        (source_int_to_vocab, target_int_to_vocab)), open('preproc.pickle', 'wb'))\n","\n","def load_preprocess():\n","    with open('preproc.pickle', mode='rb') as in_file:\n","        return pickle.load(in_file)\n","\n","\n","def enc_dec_model_inputs():\n","    \"\"\"\n","        This function creates the placeholders for the model inputs\n","    \"\"\"\n","    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n","    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n","    \n","    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n","    max_target_len = tf.reduce_max(target_sequence_length)    \n","    \n","    return inputs, targets, target_sequence_length, max_target_len\n","\n","def hyperparam_inputs():\n","    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n","    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","    \n","    return lr_rate, keep_prob\n","\n","def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n","    \"\"\"\n","    Preprocess target data for decoder, add the <GO> tag to the start of the senteces\n","    :return: Preprocessed target data\n","    \"\"\"\n","    # get '<GO>' id\n","    go_id = target_vocab_to_int['<GO>']\n","    #drop the last word which is either <PAD> or <EOS>\n","    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n","    #add the <GO> tag tp the start of the sentence\n","    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n","    \n","    return after_concat\n","\n","def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n","                   source_vocab_size, \n","                   encoding_embedding_size, cell_type):\n","    \"\"\"\n","    creates the encoder part of seq2seq learning\n","    we will use the paramters:\n","        rnn_inputs: the input to the encoding layer of the shape (Batch_size,sentence_length)\n","        rnn_size : the size of the hidden state\n","        num_layers: number of RNN layers\n","        keep_prob: the probablity of the dropout layer\n","        source_vocab_size: size of the voacabulary of the source language\n","        encoding_embedding_size: size of the embeddings for the encoder\n","        cell_type: this is the type of cell either 'LSTM' or 'GRU', \n","\n","        \n","        \n","        returns: \n","        rnn_outputs:  this are the hidden states of the RNN unit of the shape [batch_size, max_sentence_length, rnn_size]\n","        rnn_final_state: this is the state of the last rnn unit\n","\n","    :return: tuple (rnn_outputs, rnn_final_state)\n","\n","    Useful functions for implementation:\n","        tf.nn.rnn_cell.LSTMCell\n","        tf.nn.rnn_cell.GRUCell\n","        tf.nn.rnn_cell.DropoutWrapper\n","        tf.contrib.rnn.MultiRNNCell\n","        tf.nn.dynamic_rnn\n","    \"\"\"\n","    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n","                                             vocab_size=source_vocab_size, \n","                                             embed_dim=encoding_embedding_size)\n","    ######################################################################################\n","    # TODO: finish the rnn layer definition, \n","    ######################################################################################    \n","    if cell_type == 'LSTM':\n","        rnn_cell = tf.nn.rnn_cell.LSTMCell\n","    elif cell_type == 'GRU':\n","        rnn_cell = tf.nn.rnn_cell.GRUCell\n","    encoding_layers = []\n","    for i in range(0, num_layers):\n","        next_cell = rnn_cell(rnn_size)\n","        next_cell = tf.nn.rnn_cell.DropoutWrapper(next_cell, keep_prob)\n","        encoding_layers.append(next_cell)\n","    encoding_model = tf.contrib.rnn.MultiRNNCell(encoding_layers)\n","    self.encode_outputs, self.encode_state = tf.nn.dynamic_rnn(encoding_model, embed  , dtype = tf.float32)\n","    return(self.encode_outputs, self.encode_state)\n","\n","\n","def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n","                         target_sequence_length, max_target_sequence_length, \n","                         output_layer, keep_prob):\n","    \"\"\"\n","    Create a training process in decoding layer \n","    during training as we have the ground truth output. we use the ground truth output of the last layer as input \n","    to the next cell. This is done using TrainingHelper\n","\n","    args\n","    encoder_state : the state of the last RNNcell of the encoder\n","    dec_cell: the RNNcell which will be used in the decoder\n","    dec_embed_input : the decoder input mapped to the embedding space using tf.nn.embedding_lookup\n","    target_sequence_length : length of each sentence in target_language\n","    max_target_sequence_length : maximum length of sentence in target language\n","    output_layer : the dense layer used to map the rnn_otput to the target_vocaulary_size\n","    keep_prob: the probablity of the dropout layer\n","    \n","\n","    :return: BasicDecoderOutput containing training logits of the shape (batch_size,target_sequence_length,target_vocab_size)\n","    \"\"\"\n","    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n","                                             output_keep_prob=keep_prob)\n","    \n","    # for only input layer\n","    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n","                                               target_sequence_length)\n","    \n","    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n","                                              helper, \n","                                              encoder_state, \n","                                              output_layer)\n","\n","    # unrolling the decoder layer\n","    outputs, _, _ =  tf.contrib.seq2seq.dynamic_decode(decoder, \n","                                                      impute_finished=True, \n","                                                      maximum_iterations=max_target_sequence_length)\n","    return outputs\n","\n","\n","def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n","                         end_of_sequence_id, max_target_sequence_length,\n","                        output_layer, batch_size, keep_prob):\n","    \"\"\"\n","    Create a inference process in decoding layer \n","    during inference as we do not have the ground truth output. we use the decoder output of the last layer as input \n","    to the next cell. This is done using GreedyEmbeddingHelper\n","\n","    args\n","    encoder_state : the state of the last RNNcell of the encoder\n","    dec_cell: the RNNcell which will be used in the decoder\n","    dec_embeddings : a tensor to map the decoder input to the decoder embeddings\n","    start_of_sequence_id : id of the <GO> tag\n","    end_of_sequence_id : id of the <EOS> tag\n","    max_target_sequence_length : maximum length of sentence in target language\n","    output_layer : the dense layer used to map the rnn_otput to the target_vocaulary_size\n","    batch_size : the batch size of the data\n","    keep_prob: the probablity of the dropout layer\n","    \n","    \n","    :return: BasicDecoderOutput containing inference output of shape (batch_size,target_sentence_length)\n","    \"\"\"\n","    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n","                                             output_keep_prob=keep_prob)\n","    \n","    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n","                                                      tf.fill([batch_size], start_of_sequence_id), \n","                                                      end_of_sequence_id)\n","    \n","    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n","                                              helper, \n","                                              encoder_state, \n","                                              output_layer)\n","    \n","    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n","                                                      impute_finished=True, \n","                                                      maximum_iterations=max_target_sequence_length)\n","    return outputs\n","\n","def decoding_layer(dec_input, encoder_state,\n","                   target_sequence_length, max_target_sequence_length,\n","                   rnn_size,\n","                   num_layers, target_vocab_to_int, target_vocab_size,\n","                   batch_size, keep_prob, decoding_embedding_size,cell_type):\n","    \"\"\"\n","    Create decoding layer of seq2seq learning architecture.\n","    we will use the paramters:\n","        dec_input: input to the decoder of the shape (Batch_size,sentence_length)\n","        encoder_state : the state of the last RNNcell of the encoder\n","        target_sequence_length : length of each sentence in target_language\n","        max_target_sequence_length : maximum length of sentence in target language\n","        rnn_size : the size of the hidden state\n","        num_layers: number of RNN layers\n","        target_vocab_to_int : dictionary converting the target vocabulary to unique integers\n","        target_vocab_size : size of the target voceabulary\n","        batch_size : batch_size\n","        keep_prob: the probablity of the dropout layer\n","        decoding_embedding_size: size of the embeddings for the decoder\n","        cell_type: this is the type of cell either 'LSTM' or 'GRU', \n","\n","        \n","        \n","        returns: \n","        train_output: output of the type BasicDecoderOutput of the shape (batch_size,target_sequence_length,target_vocab_size)\\\n","                        these contain the logits of the output\n","        infer_output: output of the inference step of seq2seq learning. This is of the shape (batch_size,target_sequence_length)\n","\n","    :return: tuple (train_output, infer_output)\n","\n","    Useful functions for implementation:\n","        tf.nn.rnn_cell.LSTMCell\n","        tf.nn.rnn_cell.GRUCell\n","        tf.nn.embedding_lookup\n","        tf.contrib.rnn.MultiRNNCell\n","    \"\"\"\n","    ######################################################################################\n","    # TODO: finish the rnn layer definition,define the following\n","    # dec_embeddings : a tensor to map the decoder input to the decoder embeddings(used by decoding_layer_infer)\n","    # dec_embed_input : the decoder input mapped to the embedding space using tf.nn.embedding_lookup\n","    # cells : stacked RNNcells having num_layers layers\n","    ######################################################################################\n","    \n","    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n","    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n","    if cell_type == 'LSTM':\n","        rnn_cell = tf.nn.rnn_cell.LSTMCell\n","    elif cell_type == 'GRU':\n","        rnn_cell = tf.nn.rnn_cell.GRUCell\n","    decoding_layers = []\n","    for i in range(0, num_layers):\n","        next_cell = rnn_cell(rnn_size)\n","        next_cell = tf.nn.rnn_cell.DropoutWrapper(next_cell, keep_prob)\n","        decoding_layers.append(next_cell)\n","    cells = tf.contrib.rnn.MultiRNNCell(decoding_layers)\n","    ######################################################################################\n","    # END TODO:\n","    ######################################################################################\n","    \n"," \n","    #get the output of the decoder during training. This uses the TrainingHelper to get the output\n","    with tf.variable_scope(\"decode\"):\n","        output_layer = tf.layers.Dense(target_vocab_size)\n","        train_output = decoding_layer_train( encoder_state   , \n","                                            cells, \n","                                            dec_embed_input, \n","                                            target_sequence_length, \n","                                            max_target_sequence_length, \n","                                            output_layer, \n","                                            keep_prob)\n","    #get the output of the decoder during inference. This uses the GreedyEmbeddingHelper to get the output\n","    with tf.variable_scope(\"decode\", reuse=True):\n","        infer_output = decoding_layer_infer(encoder_state, \n","                                            cells, \n","                                            dec_embeddings, \n","                                            target_vocab_to_int['<GO>'], \n","                                            target_vocab_to_int['<EOS>'], \n","                                            max_target_sequence_length, \n","                                            output_layer,\n","                                            batch_size,\n","                                            keep_prob)\n","\n","    return (train_output, infer_output)\n","    \n","\n","\n","def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n","                  target_sequence_length,\n","                  max_target_sentence_length,\n","                  source_vocab_size, target_vocab_size,\n","                  enc_embedding_size, dec_embedding_size,\n","                  rnn_size, num_layers, target_vocab_to_int, cell_type):\n","    \"\"\"\n","    Build the Sequence-to-Sequence model\n","    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n","    \"\"\"\n","    enc_outputs, enc_states = encoding_layer(input_data, \n","                                             rnn_size, \n","                                             num_layers, \n","                                             keep_prob, \n","                                             source_vocab_size, \n","                                             enc_embedding_size,\n","                                             cell_type)\n","    \n","    dec_input = process_decoder_input(target_data, \n","                                      target_vocab_to_int, \n","                                      batch_size)\n","    \n","    train_output, infer_output = decoding_layer(dec_input,\n","                                               enc_states, \n","                                               target_sequence_length, \n","                                               max_target_sentence_length,\n","                                               rnn_size,\n","                                              num_layers,\n","                                              target_vocab_to_int,\n","                                              target_vocab_size,\n","                                              batch_size,\n","                                              keep_prob,\n","                                              dec_embedding_size,\n","                                              cell_type)\n","    \n","    return train_output, infer_output\n","\n","def my_optimizer(loss,grad_clip, learning_rate):\n","    '''\n","    build our optimizer\n","    Unlike previous worries of gradient vanishing problem,\n","    for some structures of rnn cells, the calculation of hidden layers' weights \n","    may lead to an \"exploding gradient\" effect where the value keeps growing.\n","    To mitigate this, we use the gradient clipping trick. Whenever the gradients are updated, \n","    they are \"clipped\" to some reasonable range (like -5 to 5) so they will never get out of this range.\n","    parameters we will use:\n","    loss, grad_clip, learning_rate\n","    :param loss: the final loss calculated by the functions\n","    :param learning_rate: (float)\n","    :param grad_clip: constraint of gradient to avoid gradient explosion\n","    we have to return:\n","    optimizer for later use\n","    '''\n","    # using clipping gradients\n","    #######################################################\n","    # TODO: implement your optimizer with gradient clipping\n","    new_optimizer = tf.train.AdamOptimizer(lr)\n","    gradients = new_optimizer.compute_gradients(cost)\n","    clipped_gradients = [(tf.clip_by_value(grad, -1*grad_clip, grad_clip), var) for grad, var in gradients if grad is not None]\n","    vals = new_optimizer.apply_gradients(clipped_gradients)\n","    return (vals)\n","    #######################################################\n","    #raise NotImplementedError\n","\n","\n","def pad_sentence_batch(sentence_batch, pad_int):\n","    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n","    max_sentence = max([len(sentence) for sentence in sentence_batch])\n","    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n","\n","def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n","    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n","    for batch_i in range(0, len(sources)//batch_size):\n","        start_i = batch_i * batch_size\n","\n","        # Slice the right amount for the batch\n","        sources_batch = sources[start_i:start_i + batch_size]\n","        targets_batch = targets[start_i:start_i + batch_size]\n","\n","        # Pad\n","        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n","        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n","\n","        # Need the lengths for the _lengths parameters\n","        pad_targets_lengths = []\n","        for target in pad_targets_batch:\n","            pad_targets_lengths.append(len(target))\n","\n","        pad_source_lengths = []\n","        for source in pad_sources_batch:\n","            pad_source_lengths.append(len(source))\n","\n","        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n","\n","\n","def get_accuracy(target, logits):\n","    \"\"\"\n","    Calculate accuracy\n","    \"\"\"\n","    max_seq = max(target.shape[1], logits.shape[1])\n","    if max_seq - target.shape[1]:\n","        target = np.pad(\n","            target,\n","            [(0,0),(0,max_seq - target.shape[1])],\n","            'constant')\n","    if max_seq - logits.shape[1]:\n","        logits = np.pad(\n","            logits,\n","            [(0,0),(0,max_seq - logits.shape[1])],\n","            'constant')\n","\n","    return np.mean(np.equal(target, logits))\n","\n","\n","def save_params(params,cell):\n","    if cell == \"LSTM\":\n","        with open('params_lstm.pickle', 'wb') as out_file:\n","            pickle.dump(params, out_file)\n","    elif cell == \"GRU\":\n","        with open('params_gru.pickle', 'wb') as out_file:\n","            pickle.dump(params, out_file)\n","\n","def load_params(cell):\n","    if cell == \"LSTM\":\n","        with open('params_lstm.pickle', mode='rb') as in_file:\n","            return pickle.load(in_file)\n","    elif cell == \"GRU\":\n","        with open('params_gru.pickle', mode='rb') as in_file:\n","            return pickle.load(in_file)\n","\n","def sentence_to_seq(sentence, vocab_to_int):\n","    results = []\n","    for word in sentence.split(\" \"):\n","        if word in vocab_to_int:\n","            results.append(vocab_to_int[word])\n","        else:\n","            results.append(vocab_to_int['<UNK>'])\n","            \n","    return results\n"],"execution_count":0,"outputs":[]}]}